{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e697d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df56009",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59faaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\"data\", train=True, download=True, transform=ToTensor())\n",
    "test_data = datasets.MNIST(\"data\", train=False, download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305de8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b72b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.fc1 = nn.Linear(28 * 28, 64)\n",
    "    self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1())\n",
    "    x = self.fc2(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0d4c40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896fc02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e18d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0986, -0.1339, -0.1010, -0.1187, -0.0802,  0.0285, -0.0141,  0.0713,\n",
       "         0.1764,  0.0260], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "out = next(iter(train_loader))\n",
    "net(out[0][0].flatten())\n",
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "98e3b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1..\n",
      "Training accuracy: 4.69\n",
      "Training loss: 2.319502115249634\n",
      "Training accuracy: 6.25\n",
      "Training loss: 2.300431728363037\n",
      "Training accuracy: 12.5\n",
      "Training loss: 2.2541399002075195\n",
      "Training accuracy: 28.12\n",
      "Training loss: 2.232858896255493\n",
      "Training accuracy: 39.06\n",
      "Training loss: 2.195103168487549\n",
      "Training accuracy: 46.88\n",
      "Training loss: 2.1859772205352783\n",
      "Training accuracy: 56.25\n",
      "Training loss: 2.123955726623535\n",
      "Training accuracy: 56.25\n",
      "Training loss: 2.1278491020202637\n",
      "Training accuracy: 56.25\n",
      "Training loss: 2.0824170112609863\n",
      "Training accuracy: 59.38\n",
      "Training loss: 2.019536256790161\n",
      "Training accuracy: 68.75\n",
      "Training loss: 1.965165138244629\n",
      "Training accuracy: 50.0\n",
      "Training loss: 1.9912686347961426\n",
      "Training accuracy: 68.75\n",
      "Training loss: 1.8935294151306152\n",
      "Training accuracy: 51.56\n",
      "Training loss: 1.9244798421859741\n",
      "Training accuracy: 57.81\n",
      "Training loss: 1.836297869682312\n",
      "Training accuracy: 60.94\n",
      "Training loss: 1.81641685962677\n",
      "Training accuracy: 53.12\n",
      "Training loss: 1.777782917022705\n",
      "Training accuracy: 53.12\n",
      "Training loss: 1.782613754272461\n",
      "Training accuracy: 54.69\n",
      "Training loss: 1.8186534643173218\n",
      "Training accuracy: 45.31\n",
      "Training loss: 1.7840226888656616\n",
      "Training accuracy: 62.5\n",
      "Training loss: 1.7075798511505127\n",
      "Training accuracy: 57.81\n",
      "Training loss: 1.7222141027450562\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.4935320615768433\n",
      "Training accuracy: 65.62\n",
      "Training loss: 1.6710412502288818\n",
      "Training accuracy: 62.5\n",
      "Training loss: 1.4876335859298706\n",
      "Training accuracy: 73.44\n",
      "Training loss: 1.52201247215271\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.4359055757522583\n",
      "Training accuracy: 76.56\n",
      "Training loss: 1.4062477350234985\n",
      "Training accuracy: 73.44\n",
      "Training loss: 1.4166247844696045\n",
      "Training accuracy: 79.69\n",
      "Training loss: 1.3012893199920654\n",
      "Training accuracy: 67.19\n",
      "Training loss: 1.3940140008926392\n",
      "Training accuracy: 71.88\n",
      "Training loss: 1.3360209465026855\n",
      "Training accuracy: 82.81\n",
      "Training loss: 1.226828694343567\n",
      "Training accuracy: 75.0\n",
      "Training loss: 1.3517694473266602\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.16146719455719\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.2502394914627075\n",
      "Training accuracy: 79.69\n",
      "Training loss: 1.128300666809082\n",
      "Training accuracy: 85.94\n",
      "Training loss: 1.1049758195877075\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.1832653284072876\n",
      "Training accuracy: 84.38\n",
      "Training loss: 1.0976440906524658\n",
      "Training accuracy: 87.5\n",
      "Training loss: 1.0410457849502563\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.9897300004959106\n",
      "Training accuracy: 79.69\n",
      "Training loss: 1.002151370048523\n",
      "Training accuracy: 75.0\n",
      "Training loss: 1.0575270652770996\n",
      "Training accuracy: 78.12\n",
      "Training loss: 1.0343167781829834\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.9408579468727112\n",
      "Training accuracy: 76.56\n",
      "Training loss: 1.01450777053833\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.9788368344306946\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.9840240478515625\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.9670060276985168\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.9152247309684753\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.9752601385116577\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.8743969798088074\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.7820295691490173\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.8756002187728882\n",
      "Training accuracy: 76.56\n",
      "Training loss: 0.984131932258606\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.7484239935874939\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.7138293385505676\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.6287317872047424\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.8155103921890259\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.6944759488105774\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.7068874835968018\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.7469246983528137\n",
      "Training accuracy: 73.44\n",
      "Training loss: 0.8858994841575623\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.7481397390365601\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.7617716193199158\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.7054721117019653\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.7979594469070435\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.657336413860321\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.6779407262802124\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.6630699634552002\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.6943184733390808\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.6483319997787476\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5570129752159119\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.620434582233429\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.7115287780761719\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.6361980438232422\n",
      "Training accuracy: 75.0\n",
      "Training loss: 0.7158987522125244\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5876667499542236\n",
      "Training accuracy: 76.56\n",
      "Training loss: 0.6125978827476501\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.5109989643096924\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.42581504583358765\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.6621894836425781\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5699852108955383\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5424751043319702\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.634300947189331\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.6549152135848999\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.6485216021537781\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.6723806858062744\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5434368252754211\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.560232400894165\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.6714323163032532\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4783974885940552\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.6778215169906616\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5183117985725403\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.475252240896225\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.537952184677124\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5420485734939575\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.6487492918968201\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5031055808067322\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.5589044094085693\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5908923745155334\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4979628324508667\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5433208346366882\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4102833867073059\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4030437767505646\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4140070080757141\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.44550418853759766\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.6115509867668152\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.39654839038848877\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4323740303516388\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.5886678695678711\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.8311946988105774\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5508916974067688\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5574581623077393\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.6082237362861633\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4973260462284088\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4379025995731354\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.47154977917671204\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.43450698256492615\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4965728521347046\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3936322033405304\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.4301111102104187\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4679315984249115\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4846822917461395\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.7151204347610474\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.6049497723579407\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5843523740768433\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.42991000413894653\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3551156222820282\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.6879412531852722\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.47692322731018066\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5170043706893921\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.32753702998161316\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4034164845943451\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.5560627579689026\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5258194208145142\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4525538682937622\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.38335511088371277\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3416175842285156\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.47178131341934204\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.40096327662467957\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5062676072120667\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3977759778499603\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3833901286125183\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5146320462226868\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.42011040449142456\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4112218916416168\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35690852999687195\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4616096615791321\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4965842664241791\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3348826766014099\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3639203906059265\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.29509952664375305\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4779975414276123\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4763180911540985\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5713913440704346\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4140617549419403\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.5176267027854919\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5534583330154419\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5822373628616333\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.40364870429039\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2810860872268677\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.43015632033348083\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3352571129798889\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4060036540031433\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3157557249069214\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4041089713573456\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2611905038356781\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5120447874069214\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5662956237792969\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4054085612297058\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5336245894432068\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3920920789241791\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3528497815132141\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.42416897416114807\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3370821475982666\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.33355554938316345\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.44211965799331665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3388364017009735\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4021950960159302\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.30780258774757385\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5355666279792786\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.27033212780952454\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5040444135665894\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3437443673610687\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.30850550532341003\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3927619159221649\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.33312737941741943\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35079535841941833\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3907508850097656\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.32801127433776855\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.39244523644447327\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3031127154827118\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5362181067466736\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3176414668560028\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4166260063648224\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5815373659133911\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.35338732600212097\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3710472583770752\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3309749662876129\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2549097537994385\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.33151495456695557\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23310787975788116\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3741935193538666\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3584563136100769\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4509303867816925\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.32367587089538574\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26450929045677185\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24611981213092804\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.46533095836639404\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4275871515274048\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.32862645387649536\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.43035751581192017\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2485007643699646\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5269510746002197\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4680714011192322\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.33714333176612854\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3801971673965454\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24766264855861664\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3650316894054413\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.32687094807624817\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.365818589925766\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2174198180437088\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4235638380050659\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.406680703163147\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26882806420326233\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34302082657814026\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3820922374725342\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3835217356681824\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3537153899669647\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29505056142807007\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4222402274608612\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5096951723098755\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3737199008464813\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3474084734916687\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.423274427652359\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32953962683677673\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4021998941898346\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4548475444316864\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3310636281967163\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.38780057430267334\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4755615293979645\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.36042752861976624\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.34140831232070923\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4148775339126587\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29499661922454834\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.28157472610473633\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2281583845615387\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.33015337586402893\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35419943928718567\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4205063581466675\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.46579208970069885\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2680460512638092\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.33919137716293335\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4113074839115143\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3755336105823517\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5327014923095703\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.49418017268180847\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.43170422315597534\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.300976037979126\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3789024353027344\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.37862876057624817\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20859204232692719\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26105213165283203\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.25714996457099915\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3639417290687561\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2381662279367447\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30220115184783936\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17861396074295044\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3408893644809723\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5278907418251038\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.419908344745636\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.43485385179519653\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.28110891580581665\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30851176381111145\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23026464879512787\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.42514288425445557\n",
      "Training accuracy: 81.25\n",
      "Training loss: 0.5490407943725586\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.3836449682712555\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.45529985427856445\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3267480134963989\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2959047257900238\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20576664805412292\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2693305015563965\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4078918397426605\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21242287755012512\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4140841066837311\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.3941248655319214\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.6282069683074951\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2950398623943329\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3397215008735657\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.40383589267730713\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3973844647407532\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5294802188873291\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.260997474193573\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.47982949018478394\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.35705164074897766\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29521188139915466\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.370883047580719\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2555062174797058\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.37707340717315674\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.28491145372390747\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3073730766773224\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.35054537653923035\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3476106822490692\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4310643672943115\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3853130042552948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2306097149848938\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4323548674583435\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5198917388916016\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2274014949798584\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.27939778566360474\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4519813060760498\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30562862753868103\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4419495761394501\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35006657242774963\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.32329869270324707\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.41174307465553284\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2562512159347534\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.48603811860084534\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3641496002674103\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.47508737444877625\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3941628932952881\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.48509716987609863\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.33264827728271484\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2603134214878082\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.32279837131500244\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2024977058172226\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.48506444692611694\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32759127020835876\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.43457019329071045\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24893975257873535\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3402702808380127\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22182166576385498\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2428618222475052\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.42769894003868103\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2721518576145172\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13239538669586182\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2546818256378174\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3540411591529846\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23073923587799072\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5062387585639954\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24467629194259644\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2866222560405731\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30068573355674744\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.36436009407043457\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4550473690032959\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25437435507774353\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.29545608162879944\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.3704512119293213\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2080206274986267\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.45402809977531433\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.320950448513031\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.47547343373298645\n",
      "Training accuracy: 78.12\n",
      "Training loss: 0.6535427570343018\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.32570821046829224\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2952098846435547\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3529229760169983\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23022162914276123\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2042696326971054\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.47426578402519226\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3878445029258728\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2820931077003479\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3097291588783264\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3086535334587097\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.34798139333724976\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3767984211444855\n",
      "Training accuracy: 79.69\n",
      "Training loss: 0.6536003351211548\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24176861345767975\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3803073763847351\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3622058928012848\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2868596017360687\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20775941014289856\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34820055961608887\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5824859142303467\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21985313296318054\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.37180960178375244\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.33520352840423584\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3317672908306122\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2857053279876709\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21305562555789948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2581792175769806\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3682442307472229\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25677669048309326\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5115637183189392\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18113850057125092\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.43336305022239685\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4751194715499878\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20571236312389374\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.33359482884407043\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4586106240749359\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4670836329460144\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2369958609342575\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19097132980823517\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.2852236330509186\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.39775964617729187\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2661843001842499\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16427937150001526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12540759146213531\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4469412565231323\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.44535085558891296\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21807587146759033\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3827981948852539\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.4624374806880951\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.28430768847465515\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4405355453491211\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34634119272232056\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.34201252460479736\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2057054340839386\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21107260882854462\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.29297885298728943\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24477876722812653\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26889219880104065\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.39423874020576477\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.31646108627319336\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20035585761070251\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5099725127220154\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15354196727275848\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5911136269569397\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2672986686229706\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.25972041487693787\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25709882378578186\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4310745298862457\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3522864580154419\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3590376079082489\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15909434854984283\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.29306086897850037\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4091617465019226\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.26002949476242065\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.452739417552948\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.38222363591194153\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.321559339761734\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4469767212867737\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3020099103450775\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2642538845539093\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22949236631393433\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3400542736053467\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3696770668029785\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.31597256660461426\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.37498539686203003\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2642763555049896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17030523717403412\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3189557194709778\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.40541890263557434\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3749929666519165\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22652609646320343\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4078280031681061\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19300906360149384\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26938092708587646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18621256947517395\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25059008598327637\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.38716521859169006\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2043122798204422\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.36716899275779724\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3836230933666229\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.47405874729156494\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.17981375753879547\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14245043694972992\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2346544712781906\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24659095704555511\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2819693386554718\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5374805927276611\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22919823229312897\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2646614909172058\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4175642132759094\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29332852363586426\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4020322263240814\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4703127145767212\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24352246522903442\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.47771814465522766\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.121849924325943\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3882063329219818\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23250578343868256\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2597310245037079\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.189650297164917\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15666191279888153\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.32251936197280884\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4006436765193939\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2234359085559845\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3624010682106018\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24282872676849365\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.31814634799957275\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3151795566082001\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2283267378807068\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2172139286994934\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23785285651683807\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.36414313316345215\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2938624620437622\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2581256926059723\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15326860547065735\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.26782965660095215\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21009284257888794\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2997351586818695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13905081152915955\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2792641222476959\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3481806814670563\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1798795908689499\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.16402937471866608\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23299629986286163\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2779456675052643\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20384472608566284\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.36671584844589233\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24496661126613617\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4602046012878418\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30242836475372314\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.28388434648513794\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3327848017215729\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3166642487049103\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21392317116260529\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25456756353378296\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.46149730682373047\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.27805471420288086\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20751003921031952\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20746299624443054\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16158944368362427\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27396395802497864\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2453230768442154\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3517366051673889\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.42728766798973083\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2519540786743164\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2477097511291504\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14391230046749115\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2715212404727936\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.32149097323417664\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.436667799949646\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19106321036815643\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.35464394092559814\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.215265154838562\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2829887568950653\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1960907131433487\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16815423965454102\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.34165114164352417\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22489529848098755\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.304580420255661\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23178240656852722\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2633497714996338\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2951377034187317\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18269021809101105\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23045748472213745\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.40404945611953735\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21401949226856232\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.314383864402771\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26259300112724304\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2983510494232178\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3604511022567749\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.48462966084480286\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16575506329536438\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23178093135356903\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.352379709482193\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23179402947425842\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24171069264411926\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.330742746591568\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19334900379180908\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24845868349075317\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2621469795703888\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3103025257587433\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19367799162864685\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2840188145637512\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24262511730194092\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.31696009635925293\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1784626841545105\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.46989327669143677\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.6069065928459167\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2461097091436386\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3035736382007599\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.516645610332489\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22048360109329224\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2887137234210968\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18245908617973328\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3442017734050751\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09505907446146011\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27401190996170044\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3912526071071625\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25362128019332886\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19763493537902832\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20885173976421356\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4521854519844055\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2688579857349396\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3071756064891815\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14995083212852478\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3085799217224121\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3413965702056885\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.35441577434539795\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2583419680595398\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3778098523616791\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17187371850013733\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.34436970949172974\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2116614729166031\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4515254497528076\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3191640079021454\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24945485591888428\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2514358460903168\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10034358501434326\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26627108454704285\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.40440836548805237\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19497250020503998\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11944011598825455\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27912306785583496\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1932620257139206\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3766220211982727\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19241762161254883\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.31004616618156433\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34262850880622864\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19075235724449158\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2977742552757263\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.2785969376564026\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2792639434337616\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.37482118606567383\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4107690155506134\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18562038242816925\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24829743802547455\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20874810218811035\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3287164568901062\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2366732507944107\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2204694151878357\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20208138227462769\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.5431272983551025\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32427865266799927\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.22940587997436523\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14520227909088135\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11710742115974426\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.215702623128891\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18158817291259766\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4791417419910431\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4153827428817749\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.29215294122695923\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34716397523880005\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.21891376376152039\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24593546986579895\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13649088144302368\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22320611774921417\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.33744016289711\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18934892117977142\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16023465991020203\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5176660418510437\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2454838752746582\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23190152645111084\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3177817165851593\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13137100636959076\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23782122135162354\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34148502349853516\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23894861340522766\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.36909663677215576\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17854256927967072\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4108193814754486\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.41108444333076477\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18169669806957245\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3390600085258484\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26204684376716614\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23648841679096222\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16461694240570068\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24885274469852448\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25080057978630066\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4401208162307739\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21831479668617249\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3031524121761322\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12473311275243759\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.30569908022880554\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2613758444786072\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2528504729270935\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.31579121947288513\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10619475692510605\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2396562397480011\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25248634815216064\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20061995089054108\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20258918404579163\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.44602063298225403\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4066016376018524\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19718216359615326\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2504134476184845\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3650025427341461\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29602643847465515\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2912822663784027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13385319709777832\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26568278670310974\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2106151580810547\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.33368998765945435\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3481639325618744\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25177228450775146\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10337711125612259\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18275298178195953\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17455171048641205\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19114206731319427\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2961188554763794\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.31868356466293335\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5116809606552124\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2682725787162781\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3960341513156891\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2654701769351959\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16869862377643585\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.30067792534828186\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3804977536201477\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23875999450683594\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4288759231567383\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.31702062487602234\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2113083451986313\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3228503167629242\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2577788829803467\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2825756072998047\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5000467896461487\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23373845219612122\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22778728604316711\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24957898259162903\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3914410173892975\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22051922976970673\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11647854000329971\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25473228096961975\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4001316428184509\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.3232426047325134\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.235226571559906\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3685508072376251\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.31926092505455017\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25421950221061707\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.30774247646331787\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3203796446323395\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.33324193954467773\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10135798901319504\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2906198501586914\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13115863502025604\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21426621079444885\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.35666385293006897\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24152344465255737\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13437601923942566\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.475570410490036\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11762835085391998\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1610192358493805\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.21735256910324097\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3528496325016022\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2984777092933655\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1616305708885193\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2513461709022522\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.37569186091423035\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22437115013599396\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22069010138511658\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09768103063106537\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15572743117809296\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19722361862659454\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2283499538898468\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25058847665786743\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29513779282569885\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3907064199447632\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3029029667377472\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16113421320915222\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.38716933131217957\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07803734391927719\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.08156704902648926\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19998104870319366\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3175419270992279\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27518612146377563\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.39023706316947937\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2705204486846924\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.31425124406814575\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.32477715611457825\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.39047521352767944\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17414870858192444\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15475597977638245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15161250531673431\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13951249420642853\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10655957460403442\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17557288706302643\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18379642069339752\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5420379638671875\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.33853524923324585\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17789241671562195\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32268035411834717\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.28371813893318176\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15873907506465912\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30598509311676025\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21981149911880493\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26396748423576355\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2059546411037445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.21327491104602814\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1658633053302765\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.28616097569465637\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1964786946773529\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1331297904253006\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15472514927387238\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.31771039962768555\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.238808736205101\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.4506424367427826\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.40047281980514526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2750680148601532\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.49928057193756104\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1973240226507187\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3979829251766205\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2969476878643036\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.25229960680007935\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3353247344493866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.10283856838941574\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20546218752861023\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30230963230133057\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29835206270217896\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22305308282375336\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13419252634048462\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20387783646583557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10046625882387161\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1982680708169937\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15454742312431335\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.29318955540657043\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2821497321128845\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3916011452674866\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4417070746421814\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.35234302282333374\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13007909059524536\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3223141133785248\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25092846155166626\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3289630711078644\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22240249812602997\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23656010627746582\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21224279701709747\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27024802565574646\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19200696051120758\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.36723679304122925\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22147339582443237\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26136261224746704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2360583394765854\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3206268846988678\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3258725106716156\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19919002056121826\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.212380513548851\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23071494698524475\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1366497278213501\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2727620005607605\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23987899720668793\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2448616921901703\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2788699269294739\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15285705029964447\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.31490591168403625\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2874816358089447\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3509483337402344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08307018876075745\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2434266358613968\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15389016270637512\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19517606496810913\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2354418933391571\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1546074002981186\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19102810323238373\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2915950417518616\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2832350730895996\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.28125327825546265\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26707956194877625\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.38807904720306396\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.34344282746315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09469098597764969\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2740941643714905\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.299861878156662\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07397489249706268\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29019811749458313\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13172627985477448\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3499361574649811\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18671366572380066\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15796102583408356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19825677573680878\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.457032173871994\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3136882185935974\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3744262456893921\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1662484109401703\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35162290930747986\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2625734210014343\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25035765767097473\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.39783474802970886\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2019987404346466\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.5032327175140381\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2645037770271301\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22047068178653717\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1329064816236496\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.38991597294807434\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13300736248493195\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.34602928161621094\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.38467925786972046\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17482705414295197\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26814672350883484\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.2942846119403839\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2957901656627655\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3327012062072754\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.45030149817466736\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0929594412446022\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15088346600532532\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20955753326416016\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20974189043045044\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15195831656455994\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3727101981639862\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24416391551494598\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.33148637413978577\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18209253251552582\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23178333044052124\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3532389998435974\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15768757462501526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1753719002008438\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20554183423519135\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24734477698802948\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2010929435491562\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.11952782422304153\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19799208641052246\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22012171149253845\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19260676205158234\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18748493492603302\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23594632744789124\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16071736812591553\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2582840323448181\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3746407926082611\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20631887018680573\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2947855293750763\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3330700397491455\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2007877081632614\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4321506917476654\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12550896406173706\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.34642308950424194\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24560853838920593\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21088428795337677\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20363062620162964\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18716521561145782\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.33537018299102783\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3016487956047058\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2682517468929291\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17241518199443817\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.11447073519229889\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30820178985595703\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22461000084877014\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2840676009654999\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21768541634082794\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3191761374473572\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.262576162815094\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.4402239918708801\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2363218367099762\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.26499027013778687\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27909576892852783\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2687565088272095\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2829611301422119\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.27506381273269653\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2140430510044098\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2877710163593292\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1954384297132492\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15061233937740326\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14798693358898163\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.18803375959396362\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07277796417474747\n",
      "Validation accuracy: 93.82\n",
      "\n",
      "Epoch 2..\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20478153228759766\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17069290578365326\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2783511281013489\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2398190051317215\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19268140196800232\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2754121422767639\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19056357443332672\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12339190393686295\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19141216576099396\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1795298159122467\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20686520636081696\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1977774053812027\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17153027653694153\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21054136753082275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.137613907456398\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18005017936229706\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27213457226753235\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22683779895305634\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3482149839401245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17959441244602203\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2192307412624359\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23516741394996643\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1618109792470932\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1778203696012497\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.2892640233039856\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1519949585199356\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14267297089099884\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20197290182113647\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25897032022476196\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.308622270822525\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19725802540779114\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24364988505840302\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19519296288490295\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26266056299209595\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13209310173988342\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15024851262569427\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27187037467956543\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19227652251720428\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.241360142827034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13101132214069366\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2586469054222107\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2622390389442444\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2259802520275116\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3016585111618042\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08903061598539352\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21842683851718903\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.36225780844688416\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3386552929878235\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3143250346183777\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09471919387578964\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2906409800052643\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21089419722557068\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.28680190443992615\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26329144835472107\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16727608442306519\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1757166087627411\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15580938756465912\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14718608558177948\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2825852334499359\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18273577094078064\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12588123977184296\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16125327348709106\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3786649703979492\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2784217298030853\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12319861352443695\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2343229353427887\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3027389943599701\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.38829511404037476\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13537082076072693\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21767573058605194\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1586213856935501\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12517765164375305\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18056797981262207\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2113412469625473\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16572999954223633\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11522325128316879\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20381303131580353\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16432850062847137\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20394830405712128\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17160113155841827\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16249728202819824\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17678290605545044\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2758604884147644\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1518319845199585\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26247885823249817\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.5206357836723328\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.117286317050457\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07854074239730835\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16419386863708496\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09909210354089737\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.25376030802726746\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15270784497261047\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16174648702144623\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29065221548080444\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20587414503097534\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.21473276615142822\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06450127810239792\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.25410181283950806\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2573816776275635\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20018991827964783\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1866450160741806\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12459856271743774\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13601133227348328\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2902262508869171\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21202638745307922\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.28546056151390076\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19665689766407013\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13910561800003052\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07270511984825134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11746145784854889\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0985901728272438\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.43590104579925537\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2496139109134674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1296062469482422\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17078275978565216\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14325648546218872\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23307034373283386\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13681022822856903\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09258385747671127\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23805290460586548\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3575611412525177\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3113027811050415\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.221465602517128\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2855367660522461\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1553470343351364\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1645732820034027\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22157137095928192\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24191053211688995\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15917904675006866\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12222836166620255\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15562939643859863\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17379535734653473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1362873762845993\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20328089594841003\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17060081660747528\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08111701905727386\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22057214379310608\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22649499773979187\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1890275776386261\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17760686576366425\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17018446326255798\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.42462974786758423\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18889285624027252\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3131130635738373\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21737487614154816\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.32166793942451477\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22826682031154633\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16069991886615753\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2074127197265625\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2398790419101715\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2863410711288452\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17139388620853424\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2749841511249542\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23613786697387695\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08480894565582275\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12986521422863007\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25496160984039307\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18929241597652435\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1859588772058487\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3212261199951172\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15064136683940887\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21275070309638977\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2771541476249695\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1786031872034073\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23551307618618011\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17118218541145325\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23270706832408905\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10422717779874802\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22914956510066986\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30159762501716614\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11122164130210876\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1245676726102829\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13651399314403534\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23455239832401276\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3501584231853485\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16965852677822113\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13925185799598694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19601643085479736\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34908413887023926\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17758262157440186\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15476202964782715\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.40879225730895996\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.30894967913627625\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24476084113121033\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19403819739818573\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2185739129781723\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3491380512714386\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17923536896705627\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16514377295970917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17765769362449646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13035835325717926\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1823592483997345\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13151343166828156\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17628400027751923\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21207831799983978\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10724770277738571\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17780447006225586\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16023293137550354\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1739465743303299\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.33117443323135376\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.5464568734169006\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28130728006362915\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2698361277580261\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10920782387256622\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.18173685669898987\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18357698619365692\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.38006627559661865\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1595681607723236\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11818525195121765\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16300149261951447\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2596629858016968\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14131082594394684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10127082467079163\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.35306599736213684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21104536950588226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09468962252140045\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1263866126537323\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26912781596183777\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07483737170696259\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.36511746048927307\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2834349572658539\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13980929553508759\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1582622230052948\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1396060287952423\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22759661078453064\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.151807501912117\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1693437546491623\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12696686387062073\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16097941994667053\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12282396852970123\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1568875014781952\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13095059990882874\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1350284367799759\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3885267376899719\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19813792407512665\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13562817871570587\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23422594368457794\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15197935700416565\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18884769082069397\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24078470468521118\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.40777385234832764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09955490380525589\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1515122950077057\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08810466527938843\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3410062789916992\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23792465031147003\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09167956560850143\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.212002694606781\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.34426990151405334\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0461282879114151\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10261610150337219\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26000335812568665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14145611226558685\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19168245792388916\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11246208101511002\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18156346678733826\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13812421262264252\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3686943054199219\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20138180255889893\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17146345973014832\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17025181651115417\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07676742225885391\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1743272989988327\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21277445554733276\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.4086115062236786\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1910994052886963\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24668563902378082\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.26848602294921875\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18933197855949402\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22292307019233704\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.28747043013572693\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2632942795753479\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29721152782440186\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12145114690065384\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3968440592288971\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3885769546031952\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2460125833749771\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.342501163482666\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28319263458251953\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1765725016593933\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1507754772901535\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13159404695034027\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.23461641371250153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08733219653367996\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17872501909732819\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07165869325399399\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16854530572891235\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30478280782699585\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2810540497303009\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.33311089873313904\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2422577291727066\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0880483090877533\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28525039553642273\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11582693457603455\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2067580223083496\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16374634206295013\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26119592785835266\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18922345340251923\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1856200248003006\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26403000950813293\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.38389235734939575\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07027734816074371\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2534663677215576\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12280825525522232\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3283224403858185\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16343174874782562\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3224881589412689\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2133607417345047\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.29900380969047546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08873246610164642\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.307222843170166\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18153704702854156\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3992019593715668\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1588868945837021\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26562443375587463\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10350210964679718\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14764779806137085\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22158083319664001\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3388413190841675\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18569447100162506\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18385502696037292\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09932930767536163\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.09644811600446701\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24263769388198853\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1478797346353531\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20907482504844666\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3809441030025482\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15244369208812714\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16975751519203186\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1849142462015152\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3141772150993347\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1617809385061264\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12763871252536774\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19305162131786346\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1689283549785614\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2988283038139343\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18954552710056305\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20255813002586365\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22123630344867706\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1153663620352745\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28051650524139404\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23041686415672302\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1885746270418167\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1762978881597519\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1549888551235199\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20553790032863617\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13550730049610138\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22508440911769867\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2627314329147339\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08838430047035217\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23447446525096893\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07644058763980865\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14052142202854156\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1252475529909134\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4309091567993164\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2788245975971222\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2518874704837799\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1492462158203125\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05915618687868118\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1661287248134613\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2318369597196579\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17108680307865143\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28271499276161194\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07671349495649338\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11626534163951874\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17368027567863464\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23865368962287903\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.26712873578071594\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.29660460352897644\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19312389194965363\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24004299938678741\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17426978051662445\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1545141637325287\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18760329484939575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09684190154075623\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15407410264015198\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16061736643314362\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3319011926651001\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16708868741989136\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1611296534538269\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10773824155330658\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.28376978635787964\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2098279893398285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12190017104148865\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2629028856754303\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17482660710811615\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11803941428661346\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3325035870075226\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3578045666217804\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.49503618478775024\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.29280373454093933\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12251225858926773\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1938384771347046\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13258075714111328\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2611481547355652\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.10308454930782318\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06138792261481285\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.3908085525035858\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1948753446340561\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2817986309528351\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18834279477596283\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2263386845588684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2219637781381607\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.162248894572258\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17398761212825775\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10591701418161392\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2173367142677307\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22982344031333923\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17412063479423523\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23533479869365692\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1619679480791092\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2821747362613678\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12944693863391876\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19432589411735535\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10231360793113708\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19294744729995728\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11257153749465942\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22118628025054932\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09341023862361908\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15420541167259216\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1607215702533722\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.22627773880958557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11390427500009537\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13429075479507446\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13938109576702118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1281079202890396\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10221254825592041\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11646787822246552\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2546517848968506\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2054966539144516\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13690036535263062\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3149906396865845\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22222469747066498\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23065602779388428\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11289554089307785\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1370510756969452\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18391641974449158\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1976269632577896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12279695272445679\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06964848190546036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08828677982091904\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18939387798309326\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2538435459136963\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24601121246814728\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3098096251487732\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03952441364526749\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13688784837722778\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16463544964790344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09538134932518005\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1919376254081726\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11911098659038544\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2853209972381592\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15077388286590576\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.409496009349823\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34636008739471436\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14281228184700012\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19129157066345215\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3195124864578247\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15328171849250793\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19506609439849854\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17412753403186798\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16483575105667114\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1101391613483429\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3497283160686493\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11760720610618591\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.28191423416137695\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12388920038938522\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2579537332057953\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.35209330916404724\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1749311089515686\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20929937064647675\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15609189867973328\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21821977198123932\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16945499181747437\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13425248861312866\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2456035166978836\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18484477698802948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16875086724758148\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18933428823947906\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15112678706645966\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14061209559440613\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27152037620544434\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18589602410793304\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1256905496120453\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32477471232414246\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16844353079795837\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3358086049556732\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1591782122850418\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1868659108877182\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14468412101268768\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2592208683490753\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.37142935395240784\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07016405463218689\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03945816308259964\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24313557147979736\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0662432387471199\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11217695474624634\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13438041508197784\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12272168695926666\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15242919325828552\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09426327794790268\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16636191308498383\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34828343987464905\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13910382986068726\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22657719254493713\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07724682986736298\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21632976830005646\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16039349138736725\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.20980694890022278\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30679064989089966\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09091129153966904\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18406006693840027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11609318107366562\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3556953966617584\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1307460516691208\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.5150890350341797\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2807437479496002\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16926869750022888\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15279048681259155\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.096932053565979\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14471638202667236\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19642533361911774\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.152249276638031\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08378667384386063\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23564448952674866\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2023480236530304\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.24427102506160736\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2557920813560486\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26460862159729004\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22383429110050201\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11989102512598038\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21884331107139587\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21112093329429626\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.13491687178611755\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1036185696721077\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.35398930311203003\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3068239986896515\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09473226219415665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16135357320308685\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19012106955051422\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10687018930912018\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2781447172164917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13805894553661346\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11806151270866394\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1384204924106598\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2332419604063034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16413193941116333\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1288909912109375\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19371391832828522\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2158937007188797\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21981902420520782\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17191877961158752\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11136896163225174\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.38495996594429016\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12003812938928604\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2419072687625885\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2015269696712494\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16894271969795227\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20525246858596802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15450447797775269\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3183615803718567\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17140178382396698\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18578453361988068\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14137481153011322\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16707879304885864\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14734791219234467\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13427437841892242\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19326059520244598\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10307279974222183\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2620426416397095\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2052880972623825\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.233986496925354\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08550854027271271\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1369401216506958\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21691188216209412\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15803904831409454\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1343989223241806\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3248387575149536\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.29976657032966614\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16495990753173828\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08991558849811554\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17478492856025696\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.47575926780700684\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25316789746284485\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1133551374077797\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3077669143676758\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13060086965560913\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2715863585472107\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18901094794273376\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17582017183303833\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13449352979660034\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3825780153274536\n",
      "Training accuracy: 82.81\n",
      "Training loss: 0.38203829526901245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18435661494731903\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14842937886714935\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23099081218242645\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1715228259563446\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18698979914188385\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17733469605445862\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07511405646800995\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13148926198482513\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1721576452255249\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.27430084347724915\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15220743417739868\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22321848571300507\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18415097892284393\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12478605657815933\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24508868157863617\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2360212206840515\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14884625375270844\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21558675169944763\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2307436764240265\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1116190031170845\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1797446608543396\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17672088742256165\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34210526943206787\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14304429292678833\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3240121006965637\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3806428611278534\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0737467110157013\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19134145975112915\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12532228231430054\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.256500244140625\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15325696766376495\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20811143517494202\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.134174644947052\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20896492898464203\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28995317220687866\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24158474802970886\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24160940945148468\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14944425225257874\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.34851887822151184\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15506787598133087\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21430973708629608\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1964263617992401\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12871147692203522\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0928865522146225\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07665807008743286\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3569681644439697\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23180314898490906\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13354834914207458\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.32980334758758545\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2524230182170868\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19133296608924866\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2105303555727005\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11585425585508347\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1973528414964676\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12068130075931549\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18407492339611053\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12398350983858109\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2041284143924713\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3183623254299164\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09088969975709915\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13439619541168213\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11731233447790146\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16175252199172974\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18219776451587677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1086505874991417\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1410190910100937\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17253436148166656\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2349119931459427\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1829187572002411\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18305441737174988\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15181072056293488\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2704290747642517\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3071887791156769\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20885470509529114\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1899324208498001\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18741120398044586\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12089003622531891\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18553051352500916\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11571059376001358\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23771585524082184\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07733267545700073\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1541254073381424\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2270660251379013\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16283054649829865\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15215520560741425\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2745034992694855\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.4327751100063324\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.34546563029289246\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1704902946949005\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1814900040626526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0686667189002037\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14793574810028076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18149372935295105\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.17112427949905396\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16004301607608795\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21326300501823425\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18656213581562042\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16495725512504578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18279345333576202\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11232724785804749\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11328324675559998\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24920374155044556\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.2627553343772888\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18690016865730286\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2658296525478363\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16310453414916992\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14584405720233917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15572287142276764\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32058078050613403\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1907537579536438\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3052598834037781\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19112029671669006\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08337079733610153\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21113598346710205\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07733780145645142\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2211613804101944\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04602591693401337\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.3359649181365967\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2974908947944641\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09507960826158524\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10984642058610916\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1386485993862152\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.35793858766555786\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.269459068775177\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.28566643595695496\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2213924527168274\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2571958005428314\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1460600048303604\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.35600197315216064\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.111190065741539\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10983406752347946\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3991241157054901\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27894458174705505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06568451225757599\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17197950184345245\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.289063036441803\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09493040293455124\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2293970286846161\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12956073880195618\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1794762909412384\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14454898238182068\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.27815136313438416\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10253142565488815\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1118648499250412\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.077835313975811\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.29996418952941895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09920184314250946\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16397516429424286\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2011844366788864\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1167643740773201\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12269307672977448\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.207064688205719\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12974011898040771\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3073784410953522\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07963938266038895\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1522560566663742\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09089895337820053\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08507928252220154\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15904560685157776\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20782358944416046\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23636150360107422\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19601701200008392\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16109420359134674\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17451918125152588\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1809522956609726\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21522897481918335\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21254920959472656\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13546344637870789\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2995259165763855\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.19359494745731354\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1290413737297058\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11804454028606415\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.4082961678504944\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05308205261826515\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12913700938224792\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24221637845039368\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.24409447610378265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05338495224714279\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15927934646606445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11367153376340866\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18272919952869415\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13186314702033997\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10234075784683228\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17437423765659332\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12415776401758194\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1213131919503212\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18659187853336334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04932049661874771\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1260702759027481\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10863593220710754\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09504526853561401\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2626367211341858\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07354175299406052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07965626567602158\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18001028895378113\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09886741638183594\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07529233396053314\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15424610674381256\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22752630710601807\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11527638137340546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09548045694828033\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1874888688325882\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2149135321378708\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24548861384391785\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15531258285045624\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16427944600582123\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26633894443511963\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19995221495628357\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1921224147081375\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11224409937858582\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057482652366161346\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17103086411952972\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2194409966468811\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12203028798103333\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.26603204011917114\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17602881789207458\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30808117985725403\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15368181467056274\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15181072056293488\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10230173915624619\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1483447104692459\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19269481301307678\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18342116475105286\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0912344753742218\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21224060654640198\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16723324358463287\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20598967373371124\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3086555302143097\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3883587718009949\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20727434754371643\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1110844612121582\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.28482839465141296\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10321676731109619\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3706675171852112\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13993844389915466\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2444969266653061\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1840488165616989\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11939217150211334\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.31725528836250305\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22402220964431763\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3034655451774597\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3205937147140503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06510167568922043\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21250194311141968\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30747318267822266\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07791277766227722\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2929806411266327\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16521979868412018\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12414088845252991\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1623731553554535\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13069523870944977\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1989532709121704\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19546328485012054\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12025206536054611\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18409690260887146\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19004064798355103\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10069067031145096\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14500851929187775\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13022439181804657\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19465704262256622\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30503010749816895\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2715623080730438\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.40431660413742065\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3058735132217407\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06224605441093445\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.28955748677253723\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08669734001159668\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16142547130584717\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1181989312171936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09605240076780319\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3289651870727539\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10106395930051804\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08756927400827408\n",
      "Training accuracy: 85.94\n",
      "Training loss: 0.24903583526611328\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2651346027851105\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11272837966680527\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1611798107624054\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05863690376281738\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09751880913972855\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16366446018218994\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1793491244316101\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1595391184091568\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11134257167577744\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2655448019504547\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.31251955032348633\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08931651711463928\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10353170335292816\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30016934871673584\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17837513983249664\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09955913573503494\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.24703074991703033\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15349841117858887\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12897837162017822\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.31820905208587646\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2741120159626007\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25531741976737976\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1127525344491005\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14757905900478363\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26600444316864014\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13777831196784973\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10732310265302658\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19430844485759735\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12649227678775787\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16856956481933594\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13885484635829926\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14585793018341064\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22313882410526276\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2149280458688736\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20486317574977875\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12863291800022125\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18026523292064667\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11753489077091217\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1152089461684227\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2907107472419739\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20578700304031372\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09273078292608261\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.2511090040206909\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15497520565986633\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.14886771142482758\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2844424843788147\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19280597567558289\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24922963976860046\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13291947543621063\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13785843551158905\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1338028460741043\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1503627449274063\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13153456151485443\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15744958817958832\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10007237643003464\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2800980508327484\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.109675832092762\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.4213336110115051\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3173683285713196\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10085898637771606\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19109874963760376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07810858637094498\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2764883041381836\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14007426798343658\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15411745011806488\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24090972542762756\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10014215856790543\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15960577130317688\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09784108400344849\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18426339328289032\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.18918240070343018\n",
      "Validation accuracy: 95.1\n",
      "\n",
      "Epoch 3..\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23830963671207428\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18480949103832245\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10225334763526917\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21393074095249176\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18501102924346924\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2007361501455307\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2968747913837433\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3427668809890747\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19913072884082794\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1147579550743103\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08161762356758118\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2776646614074707\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14569354057312012\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13086029887199402\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16783954203128815\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2492135614156723\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07874631136655807\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13395974040031433\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2560215890407562\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1333392709493637\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07171157747507095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1899079829454422\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.10335084795951843\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20938068628311157\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1279464066028595\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22465671598911285\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2941442131996155\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17402897775173187\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08494400978088379\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03648883476853371\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1723025143146515\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10410473495721817\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23736564815044403\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07511036098003387\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18008215725421906\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2141256332397461\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13247624039649963\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3009684383869171\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.263602614402771\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06424260884523392\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14153538644313812\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06590551882982254\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11559800803661346\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17235934734344482\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08690696954727173\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13461792469024658\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12144298851490021\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19146864116191864\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16612276434898376\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2226787954568863\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0943995863199234\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2453853338956833\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11362161487340927\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13871757686138153\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16586653888225555\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09223192185163498\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21045204997062683\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24255867302417755\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11816642433404922\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09819084405899048\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1630098521709442\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2344631850719452\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.09309421479701996\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14128954708576202\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18702295422554016\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1415705680847168\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10333068668842316\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.23699523508548737\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07047322392463684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12751947343349457\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10238393396139145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.154264435172081\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20336657762527466\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08971746265888214\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1388675719499588\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1338987946510315\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2729828953742981\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14062166213989258\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11299026757478714\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.331037700176239\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13739684224128723\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0801854059100151\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.34479740262031555\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07577583938837051\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17949068546295166\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11075626313686371\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17940039932727814\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10399973392486572\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2993890047073364\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13850195705890656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1324571669101715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05777103826403618\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1655857414007187\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1152852475643158\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16811950504779816\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13111914694309235\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23647893965244293\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08947297930717468\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3429286479949951\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09357565641403198\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04422461614012718\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10352547466754913\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.38117679953575134\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11652269214391708\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1272854506969452\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20614413917064667\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09455448389053345\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21740269660949707\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11176195740699768\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.140044704079628\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10487982630729675\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17021456360816956\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24681538343429565\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1176367998123169\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10425695031881332\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21975402534008026\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14230646193027496\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1258161962032318\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11729976534843445\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20776794850826263\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12786860764026642\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13448061048984528\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1370428055524826\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1963721215724945\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2383718490600586\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20456118881702423\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07479971647262573\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09516481310129166\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25520989298820496\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.18222184479236603\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24502398073673248\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21568897366523743\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2116028070449829\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30327582359313965\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2965332865715027\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11111088842153549\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1710202842950821\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.32224082946777344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12839414179325104\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1603996455669403\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18011850118637085\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12393698841333389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17336028814315796\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10336736589670181\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17100806534290314\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23793773353099823\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16381977498531342\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15284878015518188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1053677424788475\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10993479937314987\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18959079682826996\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16748087108135223\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17158395051956177\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.29166552424430847\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12146275490522385\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1407678872346878\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14554405212402344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13279135525226593\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06931676715612411\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2950011193752289\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05669623985886574\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08198996633291245\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2614257335662842\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1456281989812851\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2769226133823395\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15114913880825043\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17478594183921814\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04459773376584053\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13266779482364655\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11971757560968399\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2510090470314026\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07494038343429565\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19931477308273315\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05521012842655182\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04924018308520317\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12050087004899979\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14073707163333893\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.23054194450378418\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10989872366189957\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1614520400762558\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22178585827350616\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1674828827381134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09847110509872437\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19727164506912231\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14601926505565643\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22671374678611755\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10253173112869263\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11764771491289139\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09796813130378723\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19720064103603363\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1582765281200409\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12889522314071655\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1849997192621231\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20697326958179474\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27973634004592896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16373398900032043\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1418648213148117\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07820692658424377\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0920761227607727\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14766208827495575\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11835809797048569\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08610859513282776\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.30269795656204224\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.071485735476017\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.29357007145881653\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10370557010173798\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10804054886102676\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1406431794166565\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1651657670736313\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3383879065513611\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17445558309555054\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2605836093425751\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16321510076522827\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12297424674034119\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14641883969306946\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.28375837206840515\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.33776113390922546\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13183213770389557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09872688353061676\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08975495398044586\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05996638163924217\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1683482676744461\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23852641880512238\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18903177976608276\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12780658900737762\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16174352169036865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10852731764316559\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17895957827568054\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2417410910129547\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10619594156742096\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12458675354719162\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20008771121501923\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2180638313293457\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.118667371571064\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16134262084960938\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20827129483222961\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08153949677944183\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21001791954040527\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.040498822927474976\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07084891200065613\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22913600504398346\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12708424031734467\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11398497968912125\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22910663485527039\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13816127181053162\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1645658016204834\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30549585819244385\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14480789005756378\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0947147011756897\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08872934430837631\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10749326646327972\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07310264557600021\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1616820991039276\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11844427138566971\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19111783802509308\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06288551539182663\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13912560045719147\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06858304888010025\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17823922634124756\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16236041486263275\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.16327571868896484\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1569565087556839\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2523314952850342\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15258842706680298\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1218004822731018\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13591720163822174\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12392119318246841\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07987489551305771\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09074130654335022\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11783792823553085\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11556167900562286\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09580149501562119\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.050843242555856705\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06136208772659302\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1621382236480713\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1471235454082489\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07152368128299713\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1554422676563263\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11322811245918274\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09206219017505646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0996403694152832\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1780441254377365\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06217716634273529\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04605595022439957\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20054832100868225\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.162416011095047\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24138294160366058\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16397802531719208\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.054626017808914185\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12699328362941742\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08207657188177109\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16665680706501007\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1469121277332306\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07247811555862427\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16830432415008545\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.137253537774086\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22747483849525452\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14592452347278595\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20190995931625366\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11904948204755783\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21209651231765747\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1535160094499588\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07292284816503525\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15107890963554382\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07006332278251648\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04601201042532921\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08102816343307495\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13997606933116913\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1935705691576004\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10091491043567657\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12742331624031067\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11145733296871185\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17650872468948364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08817286789417267\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06251650303602219\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11492417007684708\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12460000813007355\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1477806121110916\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11804114282131195\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3223501741886139\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041690029203891754\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08305619657039642\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12580221891403198\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21856214106082916\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15213876962661743\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1771768182516098\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07975063472986221\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3366965651512146\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1120990514755249\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15533502399921417\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09605351090431213\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10897844284772873\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13330960273742676\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10817146301269531\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17519041895866394\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.3581494688987732\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23774154484272003\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1477479487657547\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09894193708896637\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12611030042171478\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054070740938186646\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18305270373821259\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19879873096942902\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06469324976205826\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15092600882053375\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05398254841566086\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19525618851184845\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13875453174114227\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2218276560306549\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12005053460597992\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1385577917098999\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06280290335416794\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12121929973363876\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06724434345960617\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09652919322252274\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06502178311347961\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1336432546377182\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06521179527044296\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23517943918704987\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09973306208848953\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13067905604839325\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17755745351314545\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0954245775938034\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27075064182281494\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12160833925008774\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24156196415424347\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11246781051158905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09755025804042816\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21274025738239288\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.17024457454681396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08894825726747513\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1109815239906311\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15424907207489014\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24476030468940735\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08057942241430283\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11419233679771423\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10584305226802826\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06432929635047913\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038360435515642166\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08972185850143433\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09161081165075302\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15857213735580444\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09042517840862274\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026173600926995277\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18030183017253876\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06513654440641403\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19279766082763672\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07010497897863388\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08084987848997116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08121351897716522\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10590925067663193\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16302713751792908\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15085195004940033\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10582564026117325\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.224260151386261\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11298183351755142\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23744897544384003\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1868063062429428\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13975036144256592\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1203698143362999\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13403593003749847\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08194345980882645\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2184712141752243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09423009306192398\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2759830355644226\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17101554572582245\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11894330382347107\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17162875831127167\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20672711730003357\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06828931719064713\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2180279791355133\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19010332226753235\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11932236701250076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09451030194759369\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05555250495672226\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03295617550611496\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17907078564167023\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1295185536146164\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1261419802904129\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19517453014850616\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2600533664226532\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20594938099384308\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22785162925720215\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10883789509534836\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10224037617444992\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18233591318130493\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3156498372554779\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09529277682304382\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043967895209789276\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0780680850148201\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13233834505081177\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1682683378458023\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08018173277378082\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12693670392036438\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14287659525871277\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12966908514499664\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08634786307811737\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14678722620010376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1545840948820114\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23575115203857422\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08078793436288834\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08459201455116272\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19849076867103577\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0708087682723999\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08233176916837692\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24044984579086304\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10196582227945328\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1123233512043953\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07531195878982544\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2783791124820709\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22443796694278717\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2850760519504547\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09458000212907791\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06232660636305809\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21949289739131927\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2046801894903183\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1177743449807167\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09618764370679855\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10600865632295609\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04792065545916557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08571131527423859\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03983468562364578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09025387465953827\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.157182514667511\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08951087296009064\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06586122512817383\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0631658211350441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1116723120212555\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15382349491119385\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09582565724849701\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19590811431407928\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07054483890533447\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08075282722711563\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2827163636684418\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07513047754764557\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09418810158967972\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14407922327518463\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17617468535900116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05543210729956627\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0774616003036499\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06211701035499573\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1419495940208435\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04770968109369278\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08826474100351334\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21571588516235352\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13329298794269562\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1520867645740509\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09253016114234924\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17337533831596375\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0988892912864685\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10951372236013412\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18648657202720642\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18274575471878052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0982266291975975\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12285460531711578\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0990109071135521\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12020190060138702\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14401158690452576\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20350190997123718\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09182046353816986\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11775171756744385\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2271709144115448\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12280243635177612\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2014039009809494\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.086634561419487\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11131329089403152\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12534910440444946\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.26152563095092773\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12368506193161011\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15677255392074585\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07884697616100311\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16703514754772186\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13596373796463013\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10458517074584961\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025328490883111954\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16680556535720825\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18144048750400543\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0624130554497242\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11294447630643845\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1284438967704773\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08971267938613892\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09373530745506287\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11962438374757767\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09291380643844604\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.175437793135643\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08616291731595993\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03230201452970505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07118050754070282\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1223076730966568\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15907391905784607\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15630163252353668\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20048850774765015\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1688467264175415\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1766221821308136\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.13391323387622833\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22290626168251038\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09629358351230621\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13422003388404846\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16843363642692566\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.12500840425491333\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2416696548461914\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2464737445116043\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17842897772789001\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3539941608905792\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16132383048534393\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11950215697288513\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13225285708904266\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08476383984088898\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.242097407579422\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12252814322710037\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16102105379104614\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20583128929138184\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10817098617553711\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12544596195220947\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12994582951068878\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1160702332854271\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.14397913217544556\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17719200253486633\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08987081050872803\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05687509849667549\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12407936155796051\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16978193819522858\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.36248382925987244\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20703667402267456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07348658889532089\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06807774305343628\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07110816240310669\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12677377462387085\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07058555632829666\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13578611612319946\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16767488420009613\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16674183309078217\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3550746440887451\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11475840210914612\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05990854650735855\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12700560688972473\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16495773196220398\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11638137698173523\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0985388308763504\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3008083999156952\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06251958757638931\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11213859915733337\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05708395689725876\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20435117185115814\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1704566478729248\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1369531899690628\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13481956720352173\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17181646823883057\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20087796449661255\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3017010986804962\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11944358795881271\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07552534341812134\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.058477699756622314\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08337519317865372\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06432683765888214\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0732952430844307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1025945171713829\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.09349987655878067\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11049731820821762\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08586597442626953\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05024094507098198\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2431705743074417\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.195978045463562\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3966751992702484\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055815305560827255\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.15148001909255981\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09662865847349167\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14364208281040192\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11811061948537827\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23804506659507751\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18615508079528809\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13950353860855103\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2113458663225174\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19886335730552673\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10936211794614792\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2453305870294571\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1947147250175476\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11348140984773636\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1354578584432602\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1845100224018097\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1481790691614151\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08140414208173752\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23929481208324432\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2538377344608307\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18941688537597656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1568751186132431\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14235134422779083\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12063893675804138\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11857776343822479\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12378653883934021\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1222192719578743\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22182559967041016\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1343952715396881\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19183488190174103\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08325580507516861\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12524954974651337\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19804002344608307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09304914623498917\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24640116095542908\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06790876388549805\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0635083019733429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10643605142831802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060983844101428986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.09159155189990997\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21444903314113617\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12658478319644928\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21500352025032043\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1787586659193039\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05781023949384689\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14140604436397552\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19138821959495544\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16918231546878815\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05596475303173065\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16086506843566895\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08175141364336014\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07346490770578384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09528637677431107\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06133536994457245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2826918959617615\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1437678337097168\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1521846204996109\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13827620446681976\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059182994067668915\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1131809800863266\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10663876682519913\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13405588269233704\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.240676611661911\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15924206376075745\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19759316742420197\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16398487985134125\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1704905480146408\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09369845688343048\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28028377890586853\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10283264517784119\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04501232132315636\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14867575466632843\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10030890256166458\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.19210650026798248\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19659140706062317\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20429755747318268\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3586602807044983\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18595045804977417\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06682947278022766\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11792784929275513\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08196252584457397\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08495689183473587\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08535396307706833\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11438919603824615\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13746050000190735\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06404218822717667\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.26594117283821106\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09067689627408981\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0792466402053833\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23239119350910187\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13998042047023773\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2284420132637024\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11982517689466476\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1279279738664627\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09650663286447525\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18281514942646027\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17258667945861816\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1056564450263977\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11098143458366394\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13257353007793427\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03511041775345802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07267867773771286\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09460213035345078\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16885842382907867\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08479075133800507\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20854710042476654\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08404040336608887\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07492225617170334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09986075758934021\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.30647480487823486\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.219181090593338\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07923851162195206\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13436004519462585\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2119506597518921\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06473938375711441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0656387060880661\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16669541597366333\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13347366452217102\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08211500942707062\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17415618896484375\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1965884268283844\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15963801741600037\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0637841746211052\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.35821524262428284\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11625877767801285\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08771175146102905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12293379753828049\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1631244421005249\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07763132452964783\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21431155502796173\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15552839636802673\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09630610048770905\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16303092241287231\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20971840620040894\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14194481074810028\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17186947166919708\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1491980254650116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08413729816675186\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12469441443681717\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14787712693214417\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10816521942615509\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20417720079421997\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05058862268924713\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.23927359282970428\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2712550461292267\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29911282658576965\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24739526212215424\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05503068119287491\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06367412954568863\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18829642236232758\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10533550381660461\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1867539882659912\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09001120179891586\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14210999011993408\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09507381916046143\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1247638389468193\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22331540286540985\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06345516443252563\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09463434666395187\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15121619403362274\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10485287010669708\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05269008129835129\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1750640720129013\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15863709151744843\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19160427153110504\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1250460296869278\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10501066595315933\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16006594896316528\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12592628598213196\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21653786301612854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09613227844238281\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19071261584758759\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21016989648342133\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06959044933319092\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.32214269042015076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09967575967311859\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20498161017894745\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1585521250963211\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1747434139251709\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12696440517902374\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3484959900379181\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1702057421207428\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3308348059654236\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11090648919343948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20440620183944702\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08523175120353699\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04763362184166908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19016405940055847\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07310421764850616\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14369674026966095\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17284005880355835\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12792204320430756\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06865870207548141\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09027478098869324\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1598212867975235\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2026381492614746\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07147550582885742\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20459090173244476\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2281494289636612\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2062341868877411\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21624282002449036\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18779513239860535\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2137288749217987\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.107500359416008\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04525574669241905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07739702612161636\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09031307697296143\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0889643058180809\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23903317749500275\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27266114950180054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.088058702647686\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1676585078239441\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09694882482290268\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09581633657217026\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13195739686489105\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11618121713399887\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0803854763507843\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26621297001838684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07122643291950226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058338508009910583\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08418504148721695\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0765719935297966\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2129698395729065\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17270676791667938\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04663672298192978\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1012110635638237\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1739109754562378\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20198534429073334\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10480020195245743\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12721586227416992\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12103858590126038\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14932459592819214\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0916164219379425\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18196383118629456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07559709250926971\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.237604558467865\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17915460467338562\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14548908174037933\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10654176771640778\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13357385993003845\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04547937959432602\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.254817396402359\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19159995019435883\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08411195874214172\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13230645656585693\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12688808143138885\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09306120872497559\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2685467004776001\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0805860161781311\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15097953379154205\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1820087730884552\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16502581536769867\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09532869607210159\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09901981800794601\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0990767553448677\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08723597973585129\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24455958604812622\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20351016521453857\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04084789380431175\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08237802237272263\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25557371973991394\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.28675058484077454\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24178750813007355\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15028434991836548\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06617086380720139\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15136608481407166\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0877324789762497\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12824784219264984\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1872125267982483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11418264359235764\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08134915679693222\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1877245157957077\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15832310914993286\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.28188204765319824\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05086030811071396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05038423836231232\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15174834430217743\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2136731743812561\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09427154809236526\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10665711015462875\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07108692824840546\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1857425570487976\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11849658936262131\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1540808230638504\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1327815055847168\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08447044342756271\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1567225307226181\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07471547275781631\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05737261101603508\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1173824667930603\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1698388159275055\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20985405147075653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0641217976808548\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17540214955806732\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05343582108616829\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09105244278907776\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050891123712062836\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12482894212007523\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18922419846057892\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1064736470580101\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1313484013080597\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11470599472522736\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2311762124300003\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07070238888263702\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10043200105428696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05810557305812836\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08254331350326538\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041476763784885406\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1308458298444748\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04190099239349365\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22877870500087738\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06408758461475372\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14234159886837006\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11560598015785217\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21407462656497955\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09297002106904984\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1998685747385025\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15215426683425903\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1488797515630722\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16074171662330627\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1255635768175125\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046375077217817307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1013893187046051\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2119014412164688\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24422778189182281\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06353346258401871\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042801715433597565\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16876357793807983\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19902750849723816\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1293211132287979\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2351764440536499\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11336256563663483\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08376947045326233\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2268543690443039\n",
      "Validation accuracy: 96.1\n",
      "\n",
      "Epoch 4..\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20043738186359406\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1616845726966858\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0953965112566948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11486159265041351\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10058131814002991\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15364961326122284\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12528355419635773\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16301850974559784\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12540869414806366\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04615704342722893\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10832428932189941\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14421482384204865\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20258496701717377\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14225812256336212\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06570887565612793\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10847700387239456\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2491072714328766\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11356586962938309\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1588471680879593\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03721905127167702\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09915771335363388\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05894642695784569\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04915352538228035\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04979943111538887\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12498172372579575\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1512964516878128\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1924763321876526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07509267330169678\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07792436331510544\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06756610423326492\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1304979771375656\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22482603788375854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17718325555324554\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08492967486381531\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049681417644023895\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04312573000788689\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12849248945713043\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07499077916145325\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.053316522389650345\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09922238439321518\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07174457609653473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09002900868654251\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09021874517202377\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14341463148593903\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13649557530879974\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0727652758359909\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16681073606014252\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1268758475780487\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1039455309510231\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07429128885269165\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10027024149894714\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26671186089515686\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05887284129858017\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16461141407489777\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24139758944511414\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2008352428674698\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09239989519119263\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.19457049667835236\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11103972047567368\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0995522066950798\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10446035116910934\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11063887178897858\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09351883828639984\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14103016257286072\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059817858040332794\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15822277963161469\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07825258374214172\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1253308802843094\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06944877654314041\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11902528256177902\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23244169354438782\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09164237976074219\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09802781045436859\n",
      "Training accuracy: 84.38\n",
      "Training loss: 0.2608540952205658\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13460290431976318\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12724919617176056\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14544031023979187\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09096422046422958\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06742345541715622\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07082189619541168\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25963130593299866\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0608048215508461\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0666775181889534\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24147248268127441\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1260562390089035\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1326390653848648\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1422794610261917\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0739067867398262\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1763448864221573\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09493795037269592\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0820169672369957\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06309342384338379\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08860725909471512\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14076942205429077\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08111433684825897\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061188068240880966\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.2138248085975647\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06220416724681854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08752657473087311\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24199903011322021\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15278533101081848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10550723224878311\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23735272884368896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2863844335079193\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15999771654605865\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11745506525039673\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17938049137592316\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2140134871006012\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06018117442727089\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07195958495140076\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12912142276763916\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029656358063220978\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17819586396217346\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14939555525779724\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07218687981367111\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1026800274848938\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08306232839822769\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1044904962182045\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.175295889377594\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08688555657863617\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09175548702478409\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12157288938760757\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09192965179681778\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09226740896701813\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1774718463420868\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23564250767230988\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03899115324020386\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14488723874092102\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09430286288261414\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1853480339050293\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12654270231723785\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05658058822154999\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06789446622133255\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04527777433395386\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0923902615904808\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.16632896661758423\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19447164237499237\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08418231457471848\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0931626707315445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08344807475805283\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05201151221990585\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1288270205259323\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20233944058418274\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09358438104391098\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11473429948091507\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1718902587890625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07336866855621338\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13468553125858307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08713816106319427\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20196786522865295\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13131864368915558\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15180222690105438\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07866343855857849\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.040221571922302246\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06987880170345306\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06922411173582077\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18169498443603516\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14081798493862152\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13578397035598755\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1597951352596283\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14395356178283691\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07713499665260315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0813494548201561\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06902392953634262\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07386568933725357\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22344763576984406\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.244515061378479\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07993492484092712\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18949410319328308\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028195952996611595\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16963200271129608\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1005057543516159\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14694978296756744\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20236487686634064\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12535719573497772\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14243608713150024\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15853476524353027\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0606442429125309\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14054490625858307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1319160759449005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047952570021152496\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13799992203712463\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09557545185089111\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08542171865701675\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11192736774682999\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13448955118656158\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07717686146497726\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13692167401313782\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07937577366828918\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.169489786028862\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19659531116485596\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2537369132041931\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.249931201338768\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04511534795165062\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14645357429981232\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10413342714309692\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09641368687152863\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.059193432331085205\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04456860572099686\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08759969472885132\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09597417712211609\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07443219423294067\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053946807980537415\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19966761767864227\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1276055872440338\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11348748207092285\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07240160554647446\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05129171535372734\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14231498539447784\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13578914105892181\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042702097445726395\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.18237611651420593\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11872054636478424\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3049798905849457\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10241545736789703\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1887894719839096\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1812707632780075\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14429952204227448\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04956524074077606\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.17856813967227936\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027562903240323067\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08751031756401062\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03466104716062546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0652579665184021\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08700087666511536\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09826713800430298\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1310318410396576\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13726826012134552\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07526510953903198\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14929209649562836\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09073877334594727\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14199836552143097\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059045303612947464\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08862509578466415\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.054212432354688644\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08365680277347565\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2286021113395691\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21320366859436035\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13714593648910522\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2303437888622284\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1602928191423416\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08913426846265793\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17551137506961823\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06198495626449585\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06911171972751617\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13782478868961334\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07873337715864182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18988311290740967\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0398462638258934\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13058602809906006\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24051254987716675\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12640760838985443\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06797631084918976\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018834412097930908\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14141738414764404\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056095726788043976\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05108591169118881\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07514503598213196\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12347099184989929\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05331898108124733\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0866582915186882\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.058172374963760376\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11903589218854904\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05803580954670906\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10573439300060272\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06983493268489838\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09744232892990112\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09825141727924347\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10048734396696091\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05512306094169617\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19937092065811157\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24515300989151\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10768129676580429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07758953422307968\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0436151847243309\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09406889975070953\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05642862245440483\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01911091059446335\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09939729422330856\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08686420321464539\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.080715611577034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08387813717126846\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04116150736808777\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10176629573106766\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09249982982873917\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1714639812707901\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08190218359231949\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13628533482551575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11050405353307724\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12820880115032196\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1490839123725891\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10173451155424118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0875563994050026\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17550186812877655\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1264123022556305\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04158633574843407\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13889355957508087\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12060165405273438\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07818202674388885\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1509941667318344\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09261113405227661\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12099941819906235\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15377549827098846\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0926855280995369\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10002218931913376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17410936951637268\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10053786635398865\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.28316202759742737\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03861231356859207\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07697418332099915\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10375303030014038\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11247023195028305\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1283704936504364\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07523085176944733\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1629890650510788\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15191829204559326\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11648105829954147\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07014880329370499\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24130091071128845\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20008111000061035\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09902367740869522\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07219073921442032\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08964958786964417\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07831359654664993\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08737647533416748\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09755587577819824\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1934925615787506\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19787321984767914\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029700612649321556\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09157624840736389\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07183769345283508\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.169580340385437\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25743454694747925\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12377601861953735\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15239696204662323\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1752246618270874\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13248057663440704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11905123293399811\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15512484312057495\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04195932298898697\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11787518858909607\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07854938507080078\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12103679031133652\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08261869847774506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07706861197948456\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16298815608024597\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028910519555211067\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.084049291908741\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11150919646024704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07475003600120544\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17301170527935028\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2417314350605011\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08811210095882416\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08720663189888\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09867450594902039\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.326802521944046\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12823238968849182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11763323098421097\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06793855875730515\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1976790875196457\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09239528328180313\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0711272582411766\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03126460686326027\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2285548448562622\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.146993026137352\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19493651390075684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06251388788223267\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04136916622519493\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10984860360622406\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0806368812918663\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09392663836479187\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21693076193332672\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30220624804496765\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07262972742319107\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16992412507534027\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1181783452630043\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.33507922291755676\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2137240469455719\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06479046493768692\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2254382073879242\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10504670441150665\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14728549122810364\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.049411412328481674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07224538177251816\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13253667950630188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06843948364257812\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3030202388763428\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21874558925628662\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09020542353391647\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08126921206712723\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043820567429065704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11866067349910736\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10621203482151031\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16740742325782776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12724991142749786\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03096562810242176\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10001549869775772\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11751145869493484\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1812676042318344\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08995567262172699\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06589549034833908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0936230719089508\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14281225204467773\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19684018194675446\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059604305773973465\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07161183655261993\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08045361191034317\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04327697306871414\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10031963139772415\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2942933738231659\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03545919060707092\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10668855160474777\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07681027054786682\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2014915943145752\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15285347402095795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05869940668344498\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16035166382789612\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20882602035999298\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09411680698394775\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042159322649240494\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1162247583270073\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2407156378030777\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2544911801815033\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09422943741083145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06709547340869904\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18516051769256592\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15857577323913574\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14676405489444733\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10213405638933182\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17968466877937317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13199947774410248\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07579775154590607\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10945385694503784\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10058166831731796\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1233375146985054\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08444550633430481\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13944797217845917\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04494260624051094\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1353752166032791\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18223656713962555\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1261787861585617\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10657939314842224\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13032448291778564\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06379513442516327\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.059051450341939926\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16406308114528656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09440169483423233\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15186931192874908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1111365258693695\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06367449462413788\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.058881551027297974\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14865273237228394\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04650997370481491\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11842688918113708\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15908995270729065\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07890217751264572\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1912682205438614\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14819617569446564\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08408466726541519\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039065249264240265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15467582643032074\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15742860734462738\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04321557655930519\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026912085711956024\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13059309124946594\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08520052582025528\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21618662774562836\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20545905828475952\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08253179490566254\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1324443221092224\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22194476425647736\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13025757670402527\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14836449921131134\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14031155407428741\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11442896723747253\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053502362221479416\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1724979728460312\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09161271899938583\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05167623236775398\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1764197200536728\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22509801387786865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07436355948448181\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2030077576637268\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.18935753405094147\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10321781784296036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10415010899305344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05122051388025284\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05445706099271774\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.33537599444389343\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0710800513625145\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16400355100631714\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19910870492458344\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07629182189702988\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04147098585963249\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2147139310836792\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04263360798358917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09287501126527786\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06813140213489532\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07151274383068085\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2678218185901642\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14775891602039337\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11136403679847717\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11570550501346588\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13787305355072021\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07739017903804779\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0699150338768959\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.17714443802833557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050338324159383774\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14993073046207428\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061289232224226\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17487487196922302\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08602281659841537\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1785161793231964\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13150069117546082\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08254443109035492\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1711731106042862\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09255664795637131\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14910462498664856\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08717969059944153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09592583775520325\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11421956866979599\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1257508397102356\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1702035814523697\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09769507497549057\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0773269534111023\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02439415454864502\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09133122861385345\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05931733548641205\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.058523330837488174\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10943237692117691\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09535818547010422\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07765147089958191\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07424920797348022\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06810490041971207\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11140895634889603\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18766821920871735\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27355486154556274\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10364408046007156\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15293297171592712\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13383391499519348\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0867217481136322\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09975650906562805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06871502846479416\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12810476124286652\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042609620839357376\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07870989292860031\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030126584693789482\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09693939238786697\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08565836399793625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08497379720211029\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1336241215467453\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035182200372219086\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10458151251077652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16425800323486328\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10502802580595016\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07371759414672852\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1424267292022705\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02369021251797676\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11584434658288956\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15319378674030304\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17778274416923523\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.30037420988082886\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11095789819955826\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055527981370687485\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07290614396333694\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1296653151512146\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10132139176130295\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18819762766361237\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02451915852725506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13310164213180542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04820924252271652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06304122507572174\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08399011939764023\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1611137092113495\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2542788088321686\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031532417982816696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08294741064310074\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.2306310087442398\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04722893610596657\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08838076144456863\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11815780401229858\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07673651725053787\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05398181825876236\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20527078211307526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.063228078186512\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16025780141353607\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24658870697021484\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06017183139920235\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17905639111995697\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1392066776752472\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09858901798725128\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08264587074518204\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03494410589337349\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0701650008559227\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03474925830960274\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1799866408109665\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10067033767700195\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2565460503101349\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1320803314447403\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20252488553524017\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0692083090543747\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05865776538848877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05102619528770447\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08729520440101624\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06510650366544724\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06846422702074051\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12312725931406021\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07366945594549179\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14299356937408447\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18522107601165771\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08934608846902847\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08555883169174194\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18082217872142792\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05313415080308914\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07937072217464447\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17632944881916046\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09894153475761414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09987600147724152\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07434771209955215\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16376644372940063\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18207554519176483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06831035017967224\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0843966007232666\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22997604310512543\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2106417417526245\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08466795831918716\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.24373602867126465\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17228785157203674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05412273481488228\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08757029473781586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044486213475465775\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10558084398508072\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.052094269543886185\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10435827076435089\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11447123438119888\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08304622024297714\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1629081517457962\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05334790050983429\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04712296277284622\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11195430159568787\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025497108697891235\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1160624772310257\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.21133758127689362\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07240743190050125\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07320458441972733\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09127449989318848\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1343071460723877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.2389279007911682\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14083604514598846\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.31414419412612915\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04641114920377731\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10095561295747757\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0677584558725357\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11593549698591232\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12982140481472015\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09622619301080704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10130634903907776\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10506706684827805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059944432228803635\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2044093906879425\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07367361336946487\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21350295841693878\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14419394731521606\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06919840723276138\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06331964582204819\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09991230815649033\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07407097518444061\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10835742205381393\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16108421981334686\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10625658184289932\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05877848342061043\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12690632045269012\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10017969459295273\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11460757255554199\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13461372256278992\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1403861790895462\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07233765721321106\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20248298346996307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15122823417186737\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15400388836860657\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1290569007396698\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07680220156908035\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12940846383571625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08305316418409348\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042304158210754395\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11463648825883865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11367206275463104\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07001077383756638\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19810014963150024\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17227709293365479\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11732864379882812\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056155238300561905\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11267713457345963\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11223146319389343\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2186499983072281\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1867491751909256\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13903014361858368\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13005098700523376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13125836849212646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08645088970661163\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03422970324754715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12952706217765808\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3265085518360138\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059389855712652206\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03130583092570305\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22012214362621307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07983793318271637\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027208097279071808\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051806554198265076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09362667053937912\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02458367310464382\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05565948039293289\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.052127283066511154\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24359509348869324\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17828160524368286\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10664020478725433\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16662536561489105\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09482648223638535\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045479897409677505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038875751197338104\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05604515224695206\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09156423062086105\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0563993826508522\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19486980140209198\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06902848184108734\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13814736902713776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06655362993478775\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.207266703248024\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11510979384183884\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08768683671951294\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19964268803596497\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029265347868204117\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061311665922403336\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04115765914320946\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09432653337717056\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04542908817529678\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10895802080631256\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11737928539514542\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12019483745098114\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1194700375199318\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062323108315467834\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16148509085178375\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07529947906732559\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05644147843122482\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1107892245054245\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13368813693523407\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2331087440252304\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0633985623717308\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06757484376430511\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0649358257651329\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0880873054265976\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09767298400402069\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24885046482086182\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09872330725193024\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13427649438381195\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0630083680152893\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02694069966673851\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02622026950120926\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11351379752159119\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1910603791475296\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12901704013347626\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05258768051862717\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08380059897899628\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04226810857653618\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057478226721286774\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09649322926998138\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08277548849582672\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04536718130111694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13000202178955078\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08267667144536972\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16435210406780243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044357772916555405\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09394194930791855\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10462914407253265\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15629562735557556\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02799430675804615\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08594813942909241\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1146656721830368\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1488896757364273\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02322397567331791\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027828257530927658\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16327399015426636\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1573689728975296\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11550384759902954\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04681079834699631\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07544521987438202\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06071602553129196\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1909956932067871\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14051109552383423\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23262600600719452\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20346176624298096\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13789521157741547\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14023563265800476\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16998650133609772\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16945268213748932\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19363613426685333\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025382058694958687\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15386377274990082\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13915111124515533\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2983020544052124\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2572129964828491\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16772159934043884\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09716714173555374\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16334740817546844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04769450053572655\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10667787492275238\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08997868746519089\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08755414187908173\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06453114748001099\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18754249811172485\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016055572777986526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048622503876686096\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09915406256914139\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17812764644622803\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.058422841131687164\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06960386037826538\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15800945460796356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08086889237165451\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11721247434616089\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1665409654378891\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04376394301652908\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017176084220409393\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12893769145011902\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08176624029874802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04514963924884796\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1028972640633583\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17589539289474487\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.168230801820755\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17156554758548737\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034401778131723404\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07204927504062653\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18834984302520752\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09715015441179276\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12610168755054474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05857408791780472\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0358307808637619\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11806801706552505\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11658396571874619\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12949156761169434\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08701895922422409\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2760177552700043\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058928415179252625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037521179765462875\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11910481750965118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05210098996758461\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1976633071899414\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08966519683599472\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.24371200799942017\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11055150628089905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1021336242556572\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14722831547260284\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07006876915693283\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10842010378837585\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062317751348018646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10371562093496323\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05676686763763428\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12929557263851166\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12988795340061188\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09434822201728821\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12422238290309906\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0424833744764328\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05326172709465027\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06184126436710358\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1353849619626999\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2017921805381775\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09593674540519714\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12315411865711212\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0724596455693245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15156620740890503\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13913847506046295\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02560250647366047\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15177398920059204\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08100224286317825\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1540137678384781\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046059466898441315\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11747492849826813\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.167303204536438\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03996127471327782\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1510409265756607\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07363967597484589\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17611020803451538\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06580404192209244\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15000706911087036\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03667612373828888\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023511679843068123\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3478173017501831\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11206378042697906\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0350956991314888\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14134658873081207\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06229369342327118\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03589097037911415\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1599055975675583\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06387016177177429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12496882677078247\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10431260615587234\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07544010877609253\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15364429354667664\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039895348250865936\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.252392441034317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0930996909737587\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05855269357562065\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13680124282836914\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1069716289639473\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04619497060775757\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15234209597110748\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1500512808561325\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06369004398584366\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07301145792007446\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18612347543239594\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13621367514133453\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21871423721313477\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047476716339588165\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027842067182064056\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04055875912308693\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09595534205436707\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1736571192741394\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17229321599006653\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09791281819343567\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06888903677463531\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12054312974214554\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08987955003976822\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03412150964140892\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07130299508571625\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037288665771484375\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07645772397518158\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22532613575458527\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036246996372938156\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18719211220741272\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09764748811721802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12076997011899948\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1310366541147232\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11848382651805878\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18724983930587769\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08453255891799927\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18955443799495697\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11285034567117691\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07212334871292114\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05495074763894081\n",
      "Validation accuracy: 96.33\n",
      "\n",
      "Epoch 5..\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10011179745197296\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04907187446951866\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.30664393305778503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08688393235206604\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.048905160278081894\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18531282246112823\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2548339366912842\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08025967329740524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02179623395204544\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04526888579130173\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1326478272676468\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06531815975904465\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08050578832626343\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09938321262598038\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07533339411020279\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11947597563266754\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04420425370335579\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07105845957994461\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09035409241914749\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11888576298952103\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.26226016879081726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042963624000549316\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09731803834438324\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15772278606891632\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1671370416879654\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032700907438993454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06361772119998932\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040872205048799515\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08676008880138397\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1504114866256714\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17529922723770142\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12806463241577148\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04938485100865364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08047042042016983\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04614367336034775\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06596927344799042\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09890826046466827\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057691074907779694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12151160091161728\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061623163521289825\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09473396837711334\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05342346802353859\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13899429142475128\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045080773532390594\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17904680967330933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04683341458439827\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017104919999837875\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.18166106939315796\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032553788274526596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08838361501693726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020548639819025993\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09032126516103745\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08371099829673767\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06346975266933441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0739552229642868\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06422396749258041\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1826794594526291\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07194038480520248\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.20616352558135986\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15468370914459229\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024050666019320488\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06160484254360199\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11485980451107025\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11795982718467712\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0626421719789505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04219502583146095\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07685957849025726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02073759213089943\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17190305888652802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08171135187149048\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07271653413772583\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02188868075609207\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07322442531585693\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05541868880391121\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25530168414115906\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1817934364080429\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03396134451031685\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06036493554711342\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09195437282323837\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10963398218154907\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043508462607860565\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1500958800315857\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10612814128398895\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19009363651275635\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02934420481324196\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028862513601779938\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1043817400932312\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08938797563314438\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06945735216140747\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29206383228302\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02334722690284252\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05853365361690521\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13881182670593262\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09722758829593658\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15160617232322693\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14331288635730743\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12660174071788788\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06753867119550705\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22468747198581696\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13642537593841553\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07490915805101395\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16666442155838013\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.057711102068424225\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06212369352579117\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05434737354516983\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06959988176822662\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16041849553585052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035528913140296936\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03365505486726761\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05469191446900368\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09449508786201477\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05131771042943001\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15278221666812897\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21213123202323914\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11384646594524384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07542571425437927\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027277221903204918\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22634878754615784\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028413614258170128\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17170412838459015\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1461048722267151\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08100304752588272\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05026796832680702\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.146835058927536\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.14024998247623444\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.40164074301719666\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07489075511693954\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09745346754789352\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028242599219083786\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07860588282346725\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1301378309726715\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1250477433204651\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03479200601577759\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05269896984100342\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0527818538248539\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09619368612766266\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11253137141466141\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061888452619314194\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046364933252334595\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024022892117500305\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0711761862039566\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1250504106283188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.071543388068676\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05035918578505516\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05187000334262848\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20299485325813293\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09445647150278091\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1951148808002472\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.17203673720359802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05777253210544586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058803729712963104\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033841267228126526\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011633209884166718\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.346192866563797\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05474582314491272\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08895131200551987\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12129227817058563\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05826674401760101\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05094638839364052\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08564649522304535\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08787514269351959\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11723349988460541\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13082823157310486\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0400862880051136\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21031984686851501\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04846324771642685\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06029830500483513\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043577827513217926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04526596888899803\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11249186098575592\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09777329862117767\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15253081917762756\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08771882206201553\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06463465839624405\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.29828765988349915\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0499628446996212\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1547996997833252\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15995463728904724\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06617974489927292\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042565710842609406\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11328373104333878\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08031953126192093\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041993774473667145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08076318353414536\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09574796259403229\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23246152698993683\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08961434662342072\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.21653100848197937\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09338309615850449\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04795107617974281\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04190343618392944\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08218958228826523\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14270004630088806\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03418532386422157\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10433661192655563\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07052282989025116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08196014165878296\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04415435716509819\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24781611561775208\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1191256195306778\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09178247302770615\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.051446717232465744\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.062292926013469696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06672992557287216\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.13746638596057892\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06368808448314667\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11215972900390625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06475052982568741\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07919156551361084\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.051202524453401566\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1417495757341385\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044051337987184525\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0757819414138794\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05697454884648323\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07231733202934265\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13841816782951355\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030510418117046356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09873494505882263\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06821838766336441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0605066679418087\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10118429362773895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09352022409439087\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05170726031064987\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15690910816192627\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10222344100475311\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07136540859937668\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10487506538629532\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08601304888725281\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044608984142541885\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04881022870540619\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13193294405937195\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14415670931339264\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1222587525844574\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11236090213060379\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06993698328733444\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08217920362949371\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.052089765667915344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06211668625473976\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08821553736925125\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15196886658668518\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09385950863361359\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06879427284002304\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026093509048223495\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.21373683214187622\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06527035683393478\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11250651627779007\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0669330582022667\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1840621680021286\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04583008214831352\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.31300798058509827\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05833243578672409\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08482427895069122\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05083899199962616\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.058696843683719635\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06554581969976425\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05974685028195381\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04567290097475052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07314852625131607\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10674037784337997\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08167476207017899\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.048350799828767776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0936693400144577\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04997241124510765\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05091401934623718\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12148313969373703\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15192309021949768\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0502832867205143\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030229240655899048\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07234364002943039\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09093471616506577\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10192568600177765\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037386856973171234\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14317536354064941\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0890769436955452\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10160869359970093\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05637059360742569\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1604577749967575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06620703637599945\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1372298151254654\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04458829015493393\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20762832462787628\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044425372034311295\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05566868931055069\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11277183145284653\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1521468609571457\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07540402561426163\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04486701637506485\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0812891498208046\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02257966808974743\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19985660910606384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05805237591266632\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033897653222084045\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16367143392562866\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050268370658159256\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.178590327501297\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18682421743869781\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19831019639968872\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04710369184613228\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2240380048751831\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04927251115441322\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06376387923955917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10296723246574402\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053766198456287384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10437339544296265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08951444178819656\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19752945005893707\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08599764108657837\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14030973613262177\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06354522705078125\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05063223838806152\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10394450277090073\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17896881699562073\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1672692745923996\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06747739762067795\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023358236998319626\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09041940420866013\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06795058399438858\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.047228485345840454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043443478643894196\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.2100505828857422\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04326740652322769\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.14863494038581848\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3224679231643677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05844733119010925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0924738347530365\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02569437585771084\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05788177624344826\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02915785275399685\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09931134432554245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18246550858020782\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05263630300760269\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025553710758686066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0498494990170002\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08475031703710556\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2101365476846695\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18031710386276245\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050088852643966675\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07965149730443954\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1509777307510376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07797610759735107\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15074113011360168\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1305951327085495\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08356284350156784\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0940394178032875\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06797945499420166\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15153852105140686\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.057323046028614044\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03749024122953415\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09069851785898209\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042365893721580505\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19880720973014832\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026247037574648857\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06049572303891182\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07505792379379272\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06841249763965607\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05472402274608612\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12521685659885406\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1349218785762787\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.20223157107830048\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14384368062019348\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11095356941223145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12853538990020752\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10230538994073868\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031109487637877464\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08595136553049088\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028830835595726967\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0717507153749466\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035772424191236496\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053414877504110336\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09399577230215073\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1799432933330536\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10747266560792923\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06935127079486847\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07120122015476227\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031401488929986954\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1318245381116867\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0750565156340599\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017950311303138733\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1644248068332672\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04686210677027702\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09439098834991455\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10674398392438889\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10286474972963333\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07723976671695709\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15897981822490692\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05514248460531235\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05738654360175133\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09598590433597565\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17486946284770966\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04500016197562218\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06439957022666931\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037106841802597046\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10076037049293518\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1302279531955719\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13052159547805786\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2038809210062027\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08735284209251404\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12233135849237442\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09066975116729736\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039141591638326645\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16358496248722076\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1272759884595871\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2070389688014984\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1359015703201294\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09276259690523148\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14474651217460632\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1300090253353119\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17901454865932465\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15833789110183716\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02039591781795025\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17386536300182343\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020365102216601372\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12208330631256104\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10171131789684296\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058064691722393036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04583572596311569\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046749770641326904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05952637642621994\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07918689399957657\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05410453677177429\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057809121906757355\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0882296934723854\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2157326638698578\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039106398820877075\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03759283199906349\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09388177841901779\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13545650243759155\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1515921950340271\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03332323953509331\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05554785951972008\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11421030759811401\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08951739966869354\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1356702595949173\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08746963739395142\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011833387427031994\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03356774151325226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05189233645796776\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12468436360359192\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04927373304963112\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15189310908317566\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3124776780605316\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.049801092594861984\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0949416235089302\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1542457938194275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17410826683044434\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07957237213850021\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.27202433347702026\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19056382775306702\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16021133959293365\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0834709033370018\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.284498393535614\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10733482241630554\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1513960063457489\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08797723054885864\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06126058101654053\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10805822908878326\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.3067896068096161\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.12721309065818787\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07471783459186554\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1559494435787201\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045613281428813934\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11465400457382202\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11989831924438477\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12186721712350845\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08593323081731796\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08863251656293869\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2386230081319809\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10417360067367554\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16941437125205994\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0486777238547802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0673811212182045\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13892923295497894\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06944350153207779\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06977276504039764\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05510687083005905\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03582421690225601\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029611697420477867\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06409114599227905\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012264184653759003\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017346655949950218\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08274165540933609\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16776642203330994\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17776069045066833\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08221762627363205\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06633491069078445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07306082546710968\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16392306983470917\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1028350442647934\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1040310189127922\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14385132491588593\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1177244633436203\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0852869376540184\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.060442402958869934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06658782809972763\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14125022292137146\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06915605813264847\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09156300872564316\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13730305433273315\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2028864175081253\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09601417183876038\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14919182658195496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04065454751253128\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15813636779785156\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11734668910503387\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13305877149105072\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05897187069058418\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016494035720825195\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08119917660951614\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16014008224010468\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04526887461543083\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05029415339231491\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057614028453826904\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1460915207862854\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07658170908689499\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04470779374241829\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09668678790330887\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.13250163197517395\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08080534636974335\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18236395716667175\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024399016052484512\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04306372255086899\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1272205114364624\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08307378739118576\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09357760101556778\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11258229613304138\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057174134999513626\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.11905697733163834\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1378350406885147\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07390367984771729\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2094157487154007\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10522597283124924\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10080620646476746\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10578411817550659\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09046381711959839\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037103187292814255\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07510416954755783\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10835780203342438\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13134104013442993\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12440609186887741\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1505744308233261\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0663851648569107\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12808482348918915\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07007244229316711\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057234883308410645\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08523678779602051\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14890991151332855\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1639808714389801\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1003214567899704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057754483073949814\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0863456279039383\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06158599257469177\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.075242780148983\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05373820289969444\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1460093855857849\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027178317308425903\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03465035557746887\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023037515580654144\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06972141563892365\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.076100192964077\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08649211376905441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08933837711811066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05596065893769264\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08367765694856644\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04363715648651123\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058322373777627945\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.057040005922317505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02936234138906002\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1936604231595993\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09792185574769974\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13313809037208557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07583627849817276\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10756978392601013\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1161351427435875\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08371080458164215\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03255080804228783\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01886753737926483\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07355038821697235\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07125066965818405\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10550888627767563\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09188991039991379\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029771260917186737\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18594588339328766\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07783571630716324\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08669847995042801\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030427202582359314\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16023144125938416\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07169699668884277\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04857494682073593\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033245865255594254\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0654471144080162\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04602716118097305\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03228401020169258\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030093159526586533\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08763294667005539\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09267871081829071\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12735556066036224\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01355179026722908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09822040051221848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03316207230091095\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10416986048221588\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02664083242416382\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1099187508225441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05263672024011612\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05939440801739693\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14465974271297455\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08026572316884995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06303387880325317\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05365963652729988\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051130492240190506\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11042814701795578\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1411958634853363\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037047646939754486\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07148230075836182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07132619619369507\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07061238586902618\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1633516103029251\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07654915004968643\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15427488088607788\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04055231437087059\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06746231019496918\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17919281125068665\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05671066790819168\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1003197655081749\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10974233597517014\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17043133080005646\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22076423466205597\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07247108966112137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06651763617992401\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06541808694601059\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1834954172372818\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1006053239107132\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1471198946237564\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013018039055168629\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12689469754695892\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09971378743648529\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1626037359237671\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06403455138206482\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04040803387761116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050858039408922195\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05324626713991165\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13662780821323395\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11281516402959824\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06190909445285797\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062448013573884964\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10619384050369263\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10203203558921814\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09133803844451904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07786308228969574\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06335945427417755\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02095777541399002\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0214494951069355\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18542858958244324\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020735476166009903\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1046677678823471\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10786385834217072\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0951235294342041\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.41835954785346985\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09079501032829285\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17637017369270325\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21294699609279633\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14759975671768188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060382992029190063\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04408218711614609\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13194632530212402\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042218632996082306\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1319088339805603\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09777072072029114\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1120268702507019\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10366129875183105\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.094162717461586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05897756293416023\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049280356615781784\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10012297332286835\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11327433586120605\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10708468407392502\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11926964670419693\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04663573578000069\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12421805411577225\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03619459643959999\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12190312892198563\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07256993651390076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15727879106998444\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04455780237913132\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1100691556930542\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10370046645402908\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13109667599201202\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09102672338485718\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1256517767906189\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051700934767723083\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12104592472314835\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11353916674852371\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028907736763358116\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06749173998832703\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015563707798719406\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04589634761214256\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09188373386859894\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1018453985452652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0975499153137207\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06406982243061066\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09349089860916138\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17178326845169067\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05290095508098602\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045651406049728394\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.20839178562164307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05819161236286163\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07071486115455627\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14350812137126923\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05495327338576317\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01630817912518978\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0715281292796135\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07645465433597565\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050831738859415054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07257787883281708\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22525911033153534\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1041761040687561\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1805049180984497\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10409703105688095\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08477654308080673\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18206867575645447\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10421723127365112\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08284002542495728\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08348599076271057\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0787128135561943\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03584720566868782\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1471158117055893\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.29144176840782166\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0623268187046051\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12005961686372757\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08403107523918152\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05775519832968712\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12434282898902893\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07204852253198624\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03781377524137497\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03820965066552162\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015130728483200073\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031902216374874115\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09949333220720291\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07317202538251877\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09623241424560547\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.27478471398353577\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09622468054294586\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1485653519630432\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019822092726826668\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12397170066833496\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05377034470438957\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020904112607240677\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07019951939582825\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09232170134782791\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09476877748966217\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035363856703042984\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09077044576406479\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.095561183989048\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06528197973966599\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036723144352436066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05468633770942688\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06175374239683151\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.24318139255046844\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16021975874900818\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.25405484437942505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12308607995510101\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07821568846702576\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08436690270900726\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08150798082351685\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11405420303344727\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04667504504323006\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13081727921962738\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12075600028038025\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0872248038649559\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05474422127008438\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1277206689119339\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17369723320007324\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.133806973695755\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04142051562666893\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03858775645494461\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05028634890913963\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0747983306646347\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13673438131809235\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02935522422194481\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08923589438199997\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03196968138217926\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034091297537088394\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09334118664264679\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11227864772081375\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09325514733791351\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034558072686195374\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046807751059532166\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06158439442515373\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11455973237752914\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16076520085334778\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17614662647247314\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10517117381095886\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14041802287101746\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032024312764406204\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1123766154050827\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024877581745386124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0723094791173935\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12751394510269165\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08779378980398178\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08717166632413864\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12893129885196686\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03850900009274483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07022760808467865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050909291952848434\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09218306839466095\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07337825745344162\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056250154972076416\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07690753787755966\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09624487906694412\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1220472902059555\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0934009775519371\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10282734036445618\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05275992676615715\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09935364127159119\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05074908584356308\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03727257624268532\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07197331637144089\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02086944505572319\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06533627212047577\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012094410136342049\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0351552739739418\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06708568334579468\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028416955843567848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031155085191130638\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06775008887052536\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.27341192960739136\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04666716605424881\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18763282895088196\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035057440400123596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05728448927402496\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07703042030334473\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2029431313276291\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20386312901973724\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04907219484448433\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09534504264593124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059323836117982864\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1447172313928604\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15963387489318848\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17503437399864197\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15308798849582672\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07409889996051788\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02153698541224003\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07681766152381897\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08130089938640594\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13368754088878632\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13670846819877625\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17144887149333954\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07541295886039734\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09226694703102112\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18500272929668427\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051093269139528275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10920536518096924\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11182279884815216\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10237161815166473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1293671578168869\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08042775094509125\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1922302395105362\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13607245683670044\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.066297248005867\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05854169651865959\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03427029028534889\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08291780203580856\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05302741378545761\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.130057230591774\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0994129404425621\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14701062440872192\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057188842445611954\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09615235775709152\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03553568571805954\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0788673609495163\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0414770282804966\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2202547788619995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08076339960098267\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11098912358283997\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12457912415266037\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05569083243608475\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10742975026369095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1022128015756607\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1704186648130417\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19151310622692108\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05905527621507645\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20162537693977356\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1389058381319046\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03715260699391365\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08986472338438034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10654793679714203\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044065702706575394\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03203999996185303\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05076881870627403\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038635820150375366\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029177650809288025\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11687465757131577\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2841629385948181\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06640519201755524\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05267650634050369\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14483946561813354\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03114788606762886\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.069757841527462\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028088565915822983\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14522112905979156\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12893511354923248\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05617409944534302\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022326180711388588\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043121833354234695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09190692752599716\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05279027670621872\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07547923922538757\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039486102759838104\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13961350917816162\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047751620411872864\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06387929618358612\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08729779720306396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06954038888216019\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027438610792160034\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15696005523204803\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06723617017269135\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06026165187358856\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18726322054862976\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09403052926063538\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03626234084367752\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0815964937210083\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04755152761936188\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15921318531036377\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05645088478922844\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12396442890167236\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09174855053424835\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04633307829499245\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2601831257343292\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06848545372486115\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059760160744190216\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2183060348033905\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04532591253519058\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033983923494815826\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032407164573669434\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10624268651008606\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03326110169291496\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10385289788246155\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2273935079574585\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1251838654279709\n",
      "Validation accuracy: 96.92\n",
      "\n",
      "Epoch 6..\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09017916768789291\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037386149168014526\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0556316114962101\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018793953582644463\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06740870326757431\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.24548378586769104\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09631108492612839\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04522315785288811\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0723646804690361\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13668011128902435\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11736968904733658\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04865803197026253\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06114427000284195\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03239808231592178\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034563098102808\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11259077489376068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03527428209781647\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04920060560107231\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024250155314803123\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07937044650316238\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03749699518084526\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03168481960892677\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024379974231123924\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15102726221084595\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06054292619228363\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12273940443992615\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15390153229236603\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09849274158477783\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1207641214132309\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07445545494556427\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1229872778058052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047302041202783585\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023062679916620255\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04404819756746292\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1148204579949379\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.0994982048869133\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04271547123789787\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08900956064462662\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03807579725980759\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07065236568450928\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046562060713768005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06728994101285934\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1359773725271225\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07103552669286728\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23202145099639893\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09382900595664978\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0902823880314827\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03884206712245941\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053957898169755936\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07829318940639496\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1285436600446701\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09690552949905396\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1126941367983818\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11286284029483795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12325848639011383\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04393410682678223\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03134327009320259\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13684584200382233\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11150146275758743\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045263733714818954\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04517223685979843\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0291143748909235\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02238144725561142\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10666579753160477\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22441276907920837\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016035931184887886\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0254597719758749\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039208486676216125\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08122504502534866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033977143466472626\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2531881332397461\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025856712833046913\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039016399532556534\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11177624762058258\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10364826768636703\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1130911186337471\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11573181301355362\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.20970289409160614\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.044533003121614456\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1791306883096695\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13069960474967957\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054978352040052414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05591938644647598\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15230590105056763\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03114892728626728\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057817667722702026\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03133528307080269\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0589994452893734\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11195573955774307\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03456123545765877\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1800106316804886\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08521530777215958\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12467104941606522\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12724322080612183\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10449664294719696\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09154026955366135\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04353955760598183\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07596537470817566\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04628525674343109\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11125484108924866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04378630220890045\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10546394437551498\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1258382797241211\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06632909178733826\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1769677698612213\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05555468052625656\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039179425686597824\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06406564265489578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06962168216705322\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046417444944381714\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10072338581085205\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.062136147171258926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06992248445749283\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07659684121608734\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11201510578393936\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03266717493534088\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016759254038333893\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0971650555729866\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041661255061626434\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.190353125333786\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10021988302469254\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0653221383690834\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07685475051403046\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.048215147107839584\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22269701957702637\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05567777529358864\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15689325332641602\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0658479630947113\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15817944705486298\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17938193678855896\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06733597069978714\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07346084713935852\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0397048182785511\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03910773992538452\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017279846593737602\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05192165821790695\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03340337425470352\n",
      "Training accuracy: 89.06\n",
      "Training loss: 0.2306831181049347\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05832715705037117\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08900871872901917\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14382007718086243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06763425469398499\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054823629558086395\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06932953000068665\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09223368763923645\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05848156660795212\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08825592696666718\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0527498833835125\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09764407575130463\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1557571291923523\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05750047415494919\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0974670797586441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06683869659900665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11861855536699295\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03826143220067024\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2380649298429489\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04016193747520447\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05891035869717598\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03168371319770813\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10144894570112228\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11590316146612167\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.047084465622901917\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12778913974761963\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0494612418115139\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03907273709774017\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035578403621912\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07411190867424011\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.044164933264255524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04462802782654762\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05504687875509262\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017753787338733673\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10217663645744324\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028788750991225243\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11819520592689514\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06390216946601868\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1126905009150505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03518283739686012\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05597493052482605\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057338230311870575\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02332424931228161\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13417606055736542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041451722383499146\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08710438758134842\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03845497593283653\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1685238480567932\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03410598263144493\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06693829596042633\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10814516246318817\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02760080061852932\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2029893547296524\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07011163979768753\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11932278424501419\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06785596907138824\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1088133156299591\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030825935304164886\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11562421172857285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07471929490566254\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07771293073892593\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12259276211261749\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10963915288448334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027411125600337982\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11142752319574356\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18497416377067566\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023929283022880554\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04092603921890259\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021669896319508553\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026844514533877373\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08005114644765854\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05948293209075928\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2134392410516739\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07197581976652145\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027347227558493614\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1234244853258133\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06175557151436806\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04068780690431595\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09154931455850601\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0417080856859684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.083928182721138\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10483712702989578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1111624538898468\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05184238404035568\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08777054399251938\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03960572928190231\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07956808805465698\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14167636632919312\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02148333191871643\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04682427644729614\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08081142604351044\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06964876502752304\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17358583211898804\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06360626220703125\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11255569010972977\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11853682994842529\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.052518993616104126\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10070564597845078\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.22270436584949493\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05962775647640228\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0995221734046936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0638967975974083\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.14556853473186493\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.045251332223415375\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019948964938521385\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11352766305208206\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.089393749833107\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04414234682917595\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1650097668170929\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06181959807872772\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08814474195241928\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.151524156332016\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06405649334192276\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2855524718761444\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0720825120806694\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07298581302165985\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07837972044944763\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06826069951057434\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19734805822372437\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08720147609710693\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028616812080144882\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017653940245509148\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05749005451798439\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13398383557796478\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03613768890500069\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0380169153213501\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04352235049009323\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02736109495162964\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07904098927974701\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0732332393527031\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0429016649723053\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.22714528441429138\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056070443242788315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049243077635765076\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14805968105793\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044781383126974106\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04699045047163963\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13445091247558594\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025733130052685738\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08838797360658646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035825930535793304\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03391766548156738\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023529483005404472\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032866448163986206\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.2155735343694687\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032247886061668396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04554901272058487\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16331832110881805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06705296784639359\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061767347157001495\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14847227931022644\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12287662923336029\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03418856859207153\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022936899214982986\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14363786578178406\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07062689960002899\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1404305249452591\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06819766014814377\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05914207547903061\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02698841132223606\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0829882025718689\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13216494023799896\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07170238345861435\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08482684940099716\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12707269191741943\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04501836746931076\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1693839281797409\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13474468886852264\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14697666466236115\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.24386471509933472\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.165440171957016\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05968582257628441\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09838687628507614\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10950049012899399\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15645568072795868\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04022818058729172\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1074749007821083\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09029776602983475\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0397082082927227\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08181619644165039\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06509947031736374\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06183335557579994\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11350498348474503\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08411804586648941\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03714127093553543\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09712735563516617\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03351474553346634\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15269683301448822\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017336219549179077\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12395953387022018\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03249130770564079\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08591727912425995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03416897729039192\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12042069435119629\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11955385655164719\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05210268497467041\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05016510561108589\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02999909222126007\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09334038197994232\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2736983001232147\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20262713730335236\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18497902154922485\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17328673601150513\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12634879350662231\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1668582707643509\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14986275136470795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05669582635164261\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06917780637741089\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11064757406711578\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03179408982396126\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11121970415115356\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06668323278427124\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08899196237325668\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024889493361115456\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054667457938194275\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05919579043984413\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07672606408596039\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11514854431152344\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017496630549430847\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030144469812512398\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11561781167984009\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13454732298851013\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03084893524646759\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10166431218385696\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011852039024233818\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.052814263850450516\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15507087111473083\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028874505311250687\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0993131622672081\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022825436666607857\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13291333615779877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052119042724370956\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08840331435203552\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023539243265986443\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.3175975978374481\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021880323067307472\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10950082540512085\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08737675845623016\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07330786436796188\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06749571859836578\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16360491514205933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03726077079772949\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02936861291527748\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1151716336607933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026499705389142036\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12944908440113068\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19966383278369904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041966088116168976\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03288014978170395\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03836507350206375\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04255373030900955\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037314273416996\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1177753284573555\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07474176585674286\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09216852486133575\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03306521475315094\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17810168862342834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030447859317064285\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1700747013092041\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027891362085938454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02309800498187542\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062279850244522095\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1153952032327652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1398211419582367\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04598068818449974\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12358864396810532\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08621487766504288\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1411195993423462\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0589105598628521\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03919057175517082\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13119986653327942\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05344420671463013\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15680210292339325\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0694122388958931\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08084484189748764\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022358806803822517\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.055272769182920456\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1905680000782013\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03631990775465965\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05866774171590805\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037910573184490204\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16029256582260132\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05186588317155838\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08094371110200882\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06423379480838776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08019665628671646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05807404965162277\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.22708214819431305\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15438523888587952\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06639144569635391\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05281030386686325\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19058360159397125\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1230149120092392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030264217406511307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07628532499074936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06878486275672913\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027206595987081528\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05963240563869476\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10348843038082123\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1063527837395668\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09049206227064133\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07384298741817474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09489913284778595\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10189376771450043\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05460650101304054\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11180099844932556\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.23481851816177368\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03851430118083954\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08073744177818298\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09341992437839508\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14401255548000336\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.20317885279655457\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1295824646949768\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07973489910364151\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06973154097795486\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060582488775253296\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04555707424879074\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05211351439356804\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0712791457772255\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029474623501300812\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08454042673110962\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09749717265367508\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0618918240070343\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04082677140831947\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04639819636940956\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.05812292546033859\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04197950288653374\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023650402203202248\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01933881640434265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0656990110874176\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10290632396936417\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02702511101961136\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03017827682197094\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05530758574604988\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024116387590765953\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06057319417595863\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04977978765964508\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07460375130176544\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08135334402322769\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0808803141117096\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038302306085824966\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03776414319872856\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11211056262254715\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05645716190338135\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05385059490799904\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10747532546520233\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15652617812156677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05125739425420761\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0639251321554184\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1266995519399643\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026863837614655495\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06246853992342949\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04739781841635704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061104148626327515\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09949536621570587\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047468919306993484\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.047210462391376495\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17733174562454224\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10398369282484055\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05642067268490791\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0220667514950037\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04128284752368927\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043838270008563995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06749644875526428\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0989159345626831\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027204953134059906\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03182779252529144\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12472264468669891\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10083170980215073\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11942752450704575\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08828851580619812\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12525126338005066\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12393591552972794\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04361705854535103\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.18135684728622437\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08182847499847412\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11352039873600006\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04481035843491554\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033467795699834824\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041697908192873\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08654619753360748\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07968990504741669\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1440553367137909\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06918536126613617\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049203917384147644\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13649050891399384\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08988562226295471\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027243638411164284\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03188148885965347\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14140024781227112\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06677957624197006\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07065971940755844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1140076145529747\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05734911188483238\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057275015860795975\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12528890371322632\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0727531909942627\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08831518888473511\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09972105175256729\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05268935114145279\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16389688849449158\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032802242785692215\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035665787756443024\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03193911164999008\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07612649351358414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05756475031375885\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04650767520070076\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02754352241754532\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08919135481119156\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07442421466112137\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19132527709007263\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1237519234418869\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09665267914533615\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14457085728645325\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0852968692779541\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04486316442489624\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022203480824828148\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16498298943042755\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23207490146160126\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06873051822185516\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12060277163982391\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1113123893737793\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07713519036769867\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17871926724910736\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12350139766931534\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0900537297129631\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.169763445854187\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16412466764450073\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09287051856517792\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05479782447218895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10569759458303452\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03988683968782425\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1723174899816513\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1286619007587433\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13278834521770477\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04726410284638405\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07014118134975433\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.31215521693229675\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08024871349334717\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029739413410425186\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03620913624763489\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13117003440856934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12418581545352936\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11053982377052307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05589388310909271\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07677514851093292\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05536583811044693\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037021249532699585\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08739063143730164\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02118193544447422\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11026044934988022\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11833387613296509\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.061518602073192596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04327718913555145\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016613051295280457\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03773309662938118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06648049503564835\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.152443990111351\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08348772674798965\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061960719525814056\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06673528999090195\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.32565683126449585\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05146130546927452\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04829207435250282\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08513353765010834\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1660880297422409\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08531703799962997\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06589692831039429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12712129950523376\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03334677219390869\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0435621403157711\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.16172359883785248\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10426998883485794\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027389271184802055\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11232379823923111\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02710331603884697\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06522736698389053\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03416189178824425\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06902813911437988\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06664074957370758\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19384238123893738\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08238458633422852\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09182634949684143\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04335378482937813\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15934298932552338\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05002995580434799\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10928940027952194\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02365710213780403\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02153097465634346\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1043974980711937\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08274763822555542\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06227749213576317\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13713346421718597\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07361500710248947\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01653907261788845\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021516043692827225\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1301419883966446\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.241000235080719\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041554320603609085\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026228534057736397\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011847443878650665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11854947358369827\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01371388603001833\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035916853696107864\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06001797690987587\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12411628663539886\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048260997980833054\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03825332596898079\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025479089468717575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07958164066076279\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05975116416811943\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023751435801386833\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01862688921391964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09232090413570404\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19710499048233032\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05245903506875038\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032140571624040604\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1241534873843193\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08479761332273483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07758120447397232\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07206468284130096\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06603854894638062\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16692109405994415\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15010973811149597\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05524182692170143\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15847960114479065\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10046692192554474\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018863331526517868\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03157864138484001\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06926707923412323\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059987492859363556\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031479813158512115\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20259560644626617\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06741359829902649\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029244396835565567\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09994407743215561\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04316814988851547\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08713968098163605\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028632011264562607\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09290728718042374\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11553464829921722\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06921015679836273\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03412075340747833\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09525877237319946\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03486413508653641\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02023216523230076\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05834155157208443\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02828705497086048\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1479196697473526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07904945313930511\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03728611767292023\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03393903002142906\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11215022206306458\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08410105109214783\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04050831124186516\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08689068257808685\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13424286246299744\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017315106466412544\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037991199642419815\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06975998729467392\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056366927921772\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054813917726278305\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045900858938694\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06782891601324081\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04034079238772392\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07934049516916275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06896520406007767\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046673718839883804\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12748926877975464\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15043337643146515\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09628182649612427\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18226408958435059\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06799951195716858\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06653036177158356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05444873124361038\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05422385409474373\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16328445076942444\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11466225981712341\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03925373777747154\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11619765311479568\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03190041333436966\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1852031946182251\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11520499736070633\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15492115914821625\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0709000900387764\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13908754289150238\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.21359963715076447\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050837621092796326\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05947877839207649\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02613759972155094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06251773238182068\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0861993208527565\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09633949398994446\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1209903284907341\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034392617642879486\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08649218082427979\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.060877252370119095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04657892510294914\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12578974664211273\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20899412035942078\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03166067227721214\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18060193955898285\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18247127532958984\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046641308814287186\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02497713826596737\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01969074085354805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04220583662390709\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01585843600332737\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1388874650001526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15268252789974213\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020945385098457336\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07061363011598587\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07618559896945953\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09603312611579895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07423826307058334\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09295611828565598\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14593657851219177\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05013050511479378\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2718351483345032\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04269854351878166\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055028583854436874\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09557820856571198\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0834597796201706\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04838858172297478\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14326000213623047\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0349636934697628\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028825797140598297\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038486625999212265\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.07326869666576385\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09537231177091599\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022083871066570282\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1487952321767807\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035734307020902634\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07689378410577774\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18635490536689758\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07163608074188232\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1184595450758934\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04890008643269539\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032298408448696136\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025582505390048027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08519111573696136\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03531021997332573\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10812177509069443\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03313738852739334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040787845849990845\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06124386191368103\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05824892595410347\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04825880378484726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015821166336536407\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05352633818984032\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030600175261497498\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02224585972726345\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10047083348035812\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11359436064958572\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07366720587015152\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024120092391967773\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1502693146467209\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20383216440677643\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07208652794361115\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1344604194164276\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0645628273487091\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07091861218214035\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025051672011613846\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08315929770469666\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12458653002977371\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05985002592206001\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1182086393237114\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1428413689136505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03109532780945301\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09376809746026993\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10977800190448761\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02505422942340374\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06934930384159088\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07875656336545944\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07937049865722656\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025143148377537727\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0824572890996933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05487403646111488\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028422163799405098\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04270268231630325\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10716132074594498\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03432177007198334\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024107759818434715\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1364549845457077\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14945046603679657\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08819334954023361\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05489593371748924\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15185293555259705\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08675037324428558\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10737340897321701\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054376136511564255\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0824764147400856\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035594601184129715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06597733497619629\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15872742235660553\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024612346664071083\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06717810779809952\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037087488919496536\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08254064619541168\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028090255334973335\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04830799996852875\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12638044357299805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05433399975299835\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05258392170071602\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10246668756008148\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026766091585159302\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0486455075442791\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1134827584028244\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02501625008881092\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14947418868541718\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0189984068274498\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16958582401275635\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0319071002304554\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019504442811012268\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04336344450712204\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10808313637971878\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09649517387151718\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04528486356139183\n",
      "Training accuracy: 87.5\n",
      "Training loss: 0.3962774872779846\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01349781733006239\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02484137751162052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0997471958398819\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0888785794377327\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048141539096832275\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02580694481730461\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05289357900619507\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013155815191566944\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05632436275482178\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06539886444807053\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12615422904491425\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08240516483783722\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04233214631676674\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04274957999587059\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05026847869157791\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09427237510681152\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11074099689722061\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027939431369304657\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027070902287960052\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027833661064505577\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.29053962230682373\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09962620586156845\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07475646585226059\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10285729914903641\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11924147605895996\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08815137296915054\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09770175814628601\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11216188967227936\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14325518906116486\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13989588618278503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031351398676633835\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15179283916950226\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0419183224439621\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046995725482702255\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020680509507656097\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12097549438476562\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14301927387714386\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029398929327726364\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01874263770878315\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03181476145982742\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049360241740942\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04404594004154205\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046263437718153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05406489595770836\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1324317306280136\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02068401128053665\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07034376263618469\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14426803588867188\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1276863068342209\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0360480360686779\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045076243579387665\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03164464607834816\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09477992355823517\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03975062817335129\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09545622766017914\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08318374305963516\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0771038755774498\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028429565951228142\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08044152706861496\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0807955414056778\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1407303512096405\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030020976439118385\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05658755451440811\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053463324904441833\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12211327254772186\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04964831843972206\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06367439031600952\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10061835497617722\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09149432182312012\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.162926584482193\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024088090285658836\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06367163360118866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05293739214539528\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0573134571313858\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030421512201428413\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05404020473361015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0715278834104538\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027170350775122643\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03982185199856758\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03623181954026222\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017771121114492416\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1893802434206009\n",
      "Validation accuracy: 96.83\n",
      "\n",
      "Epoch 7..\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03153195232152939\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11465272307395935\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10997120290994644\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0595572330057621\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14315441250801086\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013310834765434265\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11609882861375809\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028624096885323524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0412832647562027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047974105924367905\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04723421856760979\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049959830939769745\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036629818379879\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024933215230703354\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05856343358755112\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12039998173713684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06515312194824219\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0529542900621891\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.17700286209583282\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1360785961151123\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17876973748207092\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08927120268344879\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04561024159193039\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04784669727087021\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.040866706520318985\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10100142657756805\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07611234486103058\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021613232791423798\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04204317927360535\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04746612161397934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05848180130124092\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036661066114902496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022377654910087585\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1745421141386032\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13277877867221832\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07545787841081619\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03976016119122505\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12126091122627258\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04397619515657425\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08046205341815948\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03190029412508011\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030849514529109\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029104318469762802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04543716460466385\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03134334459900856\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023701854050159454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0793524831533432\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04305722191929817\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03269030153751373\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062746562063694\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13413530588150024\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06640603393316269\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15004731714725494\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13438504934310913\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10895784199237823\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11434068530797958\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19497208297252655\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08044145256280899\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09937829524278641\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012706136330962181\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07995541393756866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02482464723289013\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1278255432844162\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031296778470277786\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01714363694190979\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03701873868703842\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03849661350250244\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07482675462961197\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16265152394771576\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09629036486148834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026328416541218758\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07237526774406433\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053880009800195694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10986806452274323\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18824319541454315\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03484126180410385\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04999652877449989\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06812190264463425\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08539939671754837\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0456717424094677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039999742060899734\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030710121616721153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06777157634496689\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06314432621002197\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06043264642357826\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05973773077130318\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05109909176826477\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.11870665848255157\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12524978816509247\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018099894747138023\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1062408834695816\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03968822583556175\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2971494793891907\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1163739413022995\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029766086488962173\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.19178636372089386\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06508897244930267\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044721465557813644\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.071895070374012\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04258263111114502\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048378560692071915\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02396310493350029\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02990599162876606\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11981035023927689\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.12550562620162964\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11532165855169296\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06017471104860306\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07208951562643051\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08548387885093689\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06903266161680222\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06762826442718506\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01703917235136032\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025798441842198372\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03282562643289566\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08824434876441956\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04841754958033562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058363936841487885\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045610252767801285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05389488488435745\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030673891305923462\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04488543048501015\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10053402185440063\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13565000891685486\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.045607030391693115\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04614602029323578\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06316384673118591\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04989193007349968\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10249049961566925\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09852716326713562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056996457278728485\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06915047019720078\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05784772336483002\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06583140045404434\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03112664818763733\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028345756232738495\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05754917114973068\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10243258625268936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05624512583017349\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022699177265167236\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08875008672475815\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054611723870038986\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054296743124723434\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1438513696193695\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1578352153301239\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041432447731494904\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04812418296933174\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10224045813083649\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1488269865512848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05531788989901543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04082391411066055\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06254531443119049\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02206786535680294\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033082883805036545\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06020693853497505\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05098416283726692\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02106502652168274\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04716566577553749\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05595877021551132\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06668873131275177\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1173529252409935\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04480845853686333\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025800317525863647\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16392628848552704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0560339093208313\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08794517815113068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012851768173277378\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1926596611738205\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15188127756118774\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02086489647626877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0770731195807457\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11190193891525269\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05701781436800957\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0676523819565773\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054967280477285385\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04046369716525078\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05414262041449547\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02911272644996643\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03787624090909958\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06393871456384659\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05492929741740227\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07561041414737701\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0220301765948534\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046901311725378036\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06663036346435547\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04896703362464905\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01956423930823803\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0589611753821373\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04445623233914375\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07652821391820908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06611763685941696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060936469584703445\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04428495839238167\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10570738464593887\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03074631467461586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03785661607980728\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04435783624649048\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08032000809907913\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008316637016832829\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025015737861394882\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05486545339226723\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012223229743540287\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06571030616760254\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04997491464018822\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041873760521411896\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0639035701751709\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08313075453042984\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05854122340679169\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1997641772031784\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04130033031105995\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.18753808736801147\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15162476897239685\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038360290229320526\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03538830578327179\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023321975022554398\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08266699314117432\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09075264632701874\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03914331644773483\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.049692701548337936\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021855564787983894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02628485858440399\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009535456076264381\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09880094230175018\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06781647354364395\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03324253112077713\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27439266443252563\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14988112449645996\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06701431423425674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05668702721595764\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017526617273688316\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03899446502327919\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09230806678533554\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01626192405819893\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03720546513795853\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03851601853966713\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.045420557260513306\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04847856983542442\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06913606077432632\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021750565618276596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07205682247877121\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06715940684080124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06178202107548714\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04030096158385277\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025668727234005928\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011239147745072842\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09032884240150452\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09604522585868835\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.068671315908432\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03842053562402725\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05676260218024254\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042878489941358566\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07506448775529861\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030500378459692\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03220689296722412\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04993632063269615\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11896416544914246\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07245653867721558\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036386746913194656\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04155914857983589\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0731467455625534\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09072156250476837\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0103744026273489\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1391488015651703\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10753627121448517\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07966871559619904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06089615076780319\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18696635961532593\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0365905687212944\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08141880482435226\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03885825723409653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04320233315229416\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09484586119651794\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14293690025806427\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08725124597549438\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13095620274543762\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048982974141836166\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04451308771967888\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06299546360969543\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09073223173618317\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08806303888559341\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09283842891454697\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04803634434938431\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08950492739677429\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03595154359936714\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1208656057715416\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06209355592727661\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08396580815315247\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07423754781484604\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08036649227142334\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05754825472831726\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0627431645989418\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0746164619922638\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1340862661600113\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0441250279545784\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09290625900030136\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029896873980760574\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015946203842759132\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13965220749378204\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04160036891698837\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12144750356674194\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1489550620317459\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06174236536026001\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04598476365208626\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1281890869140625\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07567229866981506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09933021664619446\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027904950082302094\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02514137700200081\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024874519556760788\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045022979378700256\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06340876966714859\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05651605501770973\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07204529643058777\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03975576162338257\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03614983707666397\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07693635672330856\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05668693408370018\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031116528436541557\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043189529329538345\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029456360265612602\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16208922863006592\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03558395057916641\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07327206432819366\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02288719266653061\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044852834194898605\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04422036185860634\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0893695205450058\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030307836830615997\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019160257652401924\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06665117293596268\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027910485863685608\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03181479871273041\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11196364462375641\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06361476331949234\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028388576582074165\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07428067177534103\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10687955468893051\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07389958947896957\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011845866218209267\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04199037328362465\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12033811211585999\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035255253314971924\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.25529661774635315\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03664691746234894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0491141676902771\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07872014492750168\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11755017191171646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0361529178917408\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08144837617874146\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07278452813625336\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09716901183128357\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04593885317444801\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11815571039915085\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08451209962368011\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0335066057741642\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22892150282859802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06845752894878387\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15328243374824524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028036518022418022\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12114354223012924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03164605051279068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016002725809812546\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03358452394604683\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02021108753979206\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016967730596661568\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04687490686774254\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011195938102900982\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08314941078424454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09377282857894897\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0819634348154068\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039401788264513016\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08695682883262634\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047218821942806244\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029419152066111565\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12243756651878357\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026400092989206314\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05597947910428047\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14416222274303436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07169545441865921\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04393918812274933\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11075963824987411\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.021892765536904335\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09715831279754639\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04194600135087967\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06841332465410233\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024401171132922173\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031405214220285416\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0382893905043602\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10699543356895447\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10907310247421265\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11188343912363052\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041848231106996536\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027304263785481453\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07553862035274506\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023236945271492004\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029302680864930153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0285418089479208\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04348517209291458\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05918430909514427\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11130337417125702\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03733048215508461\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011786391958594322\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027952659875154495\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.14696064591407776\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09817944467067719\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03559671342372894\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025885991752147675\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0797877311706543\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027663258835673332\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04137459024786949\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15300822257995605\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019387053325772285\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020483648404479027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04989808425307274\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04941563680768013\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.050291817635297775\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028680630028247833\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.021783102303743362\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10962000489234924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022624272853136063\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07746311277151108\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019712233915925026\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01782810315489769\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15922911465168\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011861594393849373\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14823052287101746\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025572488084435463\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20197877287864685\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09552779793739319\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07863955199718475\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04093918949365616\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10580015182495117\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03164788335561752\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14782245457172394\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043260447680950165\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036971256136894226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050567977130413055\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14066901803016663\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0919293537735939\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05078316852450371\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.035084810107946396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09073959290981293\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014044810086488724\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09009221196174622\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07092215865850449\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025302458554506302\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09104736149311066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04406675696372986\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05728535354137421\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12095313519239426\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08679649233818054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10152582079172134\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11130493134260178\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09031854569911957\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041752200573682785\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07125523686408997\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17699457705020905\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10380878299474716\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08308485150337219\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15077291429042816\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06658444553613663\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013277987949550152\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03575226664543152\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09500183910131454\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08106768876314163\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014985349960625172\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09014640003442764\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02310018427670002\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04042644798755646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03952842578291893\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1627153605222702\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04330230504274368\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029536882415413857\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03856157511472702\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011995358392596245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1889830082654953\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18507082760334015\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.061494164168834686\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026454448699951172\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034753378480672836\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05437923222780228\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11776678264141083\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057091813534498215\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04373034089803696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0504959374666214\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04330264404416084\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04565929248929024\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.041896753013134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03717269003391266\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.05361662805080414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0534878745675087\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04594115540385246\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12970970571041107\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0673169195652008\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.138027623295784\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02146802470088005\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011137853376567364\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12438514828681946\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035949140787124634\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06517333537340164\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029401972889900208\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032837871462106705\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03496931865811348\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11979299783706665\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11108029633760452\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06520582735538483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06590516120195389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06655258685350418\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06332811713218689\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08503454178571701\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06097976490855217\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02254209667444229\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10220001637935638\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03848125413060188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07767350226640701\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05139883980154991\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15256066620349884\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02629777416586876\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11610729247331619\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039899781346321106\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09358929097652435\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03594159334897995\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10869259387254715\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05301377922296524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01437330897897482\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03029586747288704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11231348663568497\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024456579238176346\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014328967779874802\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0623294971883297\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07116830348968506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08491919934749603\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1426832526922226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11631613969802856\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19527216255664825\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1424834430217743\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09278665482997894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03141539543867111\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06607244908809662\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13014286756515503\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12121398746967316\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1260850578546524\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05936923250555992\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07686178386211395\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13912035524845123\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01839618757367134\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056758247315883636\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10595478862524033\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09583021700382233\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04430747404694557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06800228357315063\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13666558265686035\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052169837057590485\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010791956447064877\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04113559052348137\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013631798326969147\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09432867914438248\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09711284935474396\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0883055031299591\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009864965453743935\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05866127088665962\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043767739087343216\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0496625155210495\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09460140019655228\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23936223983764648\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16941121220588684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045349180698394775\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037932559847831726\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09376928955316544\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03373301401734352\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011088777333498001\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035736020654439926\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03089231066405773\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.18706431984901428\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03736284375190735\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10955405980348587\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10493030399084091\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1554042100906372\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07091343402862549\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05774077773094177\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058021776378154755\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007365945260971785\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03720029816031456\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0389997698366642\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09766464680433273\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13285693526268005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045164432376623154\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05969245731830597\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08919097483158112\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0964633896946907\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017401406541466713\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12394276261329651\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0666532963514328\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04895731434226036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050498347729444504\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02442915365099907\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049582574516534805\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09909725189208984\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03174952417612076\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06165217608213425\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06506696343421936\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02669290453195572\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05994364619255066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06297481805086136\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03796821087598801\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06355544924736023\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11548317968845367\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07027754187583923\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05263853818178177\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09635162353515625\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018845200538635254\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04962361603975296\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01906043477356434\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021701954305171967\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018646491691470146\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010604370385408401\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0712711364030838\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09323200583457947\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03577329218387604\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0919571965932846\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043392252177000046\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10902415961027145\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04346378147602081\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08791296184062958\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15245385468006134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05179480463266373\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01919425278902054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03677738457918167\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06744828820228577\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06712248921394348\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08389760553836823\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10905224084854126\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.046835459768772125\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042017605155706406\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11968284100294113\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06949112564325333\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10870734602212906\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0744367316365242\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026547806337475777\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1323309987783432\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03901767358183861\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023865005001425743\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10895437002182007\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054708272218704224\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10381537675857544\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12801696360111237\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09617605805397034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09287222474813461\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019329791888594627\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024785015732049942\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05064446106553078\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08489397913217545\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0760132223367691\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037412095814943314\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01955588534474373\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15245816111564636\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04272260516881943\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05154723674058914\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06054268032312393\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06029141694307327\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03858549892902374\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03782939538359642\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03716803342103958\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06282372027635574\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03536937013268471\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028940357267856598\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04475574195384979\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017801538109779358\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03778944909572601\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.15487255156040192\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01795249991118908\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023826230317354202\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039765190333127975\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08031664788722992\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022502081468701363\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08697046339511871\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09568008780479431\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1615149974822998\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04699923098087311\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0851292759180069\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04866540804505348\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.135247141122818\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08062968403100967\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03933528810739517\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04978345334529877\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039588574320077896\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04761641100049019\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12375359982252121\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11683906614780426\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06591784209012985\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026545708999037743\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08121926337480545\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06382690370082855\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031045952811837196\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1527678221464157\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1298607885837555\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05600248649716377\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10941450297832489\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17657674849033356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07724770158529282\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031582076102495193\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.083164282143116\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031670358031988144\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024959996342658997\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07271688431501389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.055497005581855774\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1783083975315094\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03617379814386368\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025287233293056488\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009133641608059406\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03633144870400429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06475428491830826\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03910331428050995\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027962584048509598\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04743427038192749\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.152398481965065\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03633315488696098\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03114696964621544\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15242744982242584\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031941626220941544\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11217191815376282\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03655676543712616\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09550946205854416\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08226172626018524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03406023234128952\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08711301535367966\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10740111023187637\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10169009119272232\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.051641661673784256\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18265922367572784\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04379004240036011\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03668329492211342\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053186919540166855\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04088147357106209\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04423564672470093\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050248630344867706\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13705798983573914\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021017378196120262\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1656259298324585\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08119390159845352\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015477635897696018\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040810950100421906\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.050312042236328125\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05159913748502731\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02891276776790619\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03177524730563164\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0867585688829422\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08189257234334946\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04639174044132233\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11983691900968552\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01696363091468811\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17380085587501526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14229507744312286\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045913293957710266\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021722156554460526\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04108165577054024\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043067142367362976\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019140366464853287\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11363907903432846\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12167716026306152\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10095734894275665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13676771521568298\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08865462988615036\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1227874755859375\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05801812931895256\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054336436092853546\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03799339756369591\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08784759789705276\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0787527933716774\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019173510372638702\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.044122882187366486\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040853310376405716\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09337250888347626\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031241586431860924\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11020927131175995\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13861902058124542\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047023773193359375\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12784601747989655\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10173279047012329\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029199903830885887\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14800214767456055\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031102292239665985\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08136028796434402\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010234257206320763\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05032169818878174\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06852320581674576\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12603650987148285\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04043314978480339\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08095823228359222\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08729366958141327\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.17954540252685547\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2976507544517517\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13588693737983704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2057410180568695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05456828698515892\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06721073389053345\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01261303760111332\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05507271736860275\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1120014414191246\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036187876015901566\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06645704060792923\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03923813998699188\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13958537578582764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025506731122732162\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07968335598707199\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027844222262501717\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052686117589473724\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.22745805978775024\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09694086015224457\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02002987451851368\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10398270934820175\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10786325484514236\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0302295945584774\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08496835827827454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07648693025112152\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07446733862161636\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19469566643238068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023199627175927162\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12904202938079834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025902407243847847\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012605276890099049\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039386555552482605\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08458052575588226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08400118350982666\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044987645000219345\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10005534440279007\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04633069410920143\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03692395240068436\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2114955633878708\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037359170615673065\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08476457744836807\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04929459095001221\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0385238341987133\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06046261265873909\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1417510062456131\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04210711643099785\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02879001758992672\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15715420246124268\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09216205030679703\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011735682375729084\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16313667595386505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10179643332958221\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05682424455881119\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02948084846138954\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017970366403460503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05901220440864563\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05986349284648895\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042573753744363785\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029194282367825508\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030348623171448708\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0969603881239891\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021929502487182617\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05542919784784317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04444939270615578\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027804750949144363\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07853717356920242\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06277936697006226\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.23426717519760132\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019339077174663544\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03152574226260185\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12913140654563904\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09201306104660034\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0850275307893753\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12833286821842194\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13483460247516632\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028480425477027893\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03857552632689476\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11539848148822784\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12804552912712097\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043195899575948715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.091034434735775\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016193732619285583\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2539117634296417\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1063537672162056\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11852560192346573\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030211230739951134\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023112190887331963\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06950906664133072\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0259657371789217\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018025506287813187\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03637581318616867\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0981239527463913\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018987832590937614\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09135585278272629\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1430063247680664\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024885311722755432\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1232687458395958\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08742222189903259\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08515201508998871\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.3108139634132385\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0672123059630394\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11073067039251328\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10381446778774261\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11962099373340607\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20879040658473969\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022237006574869156\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0478954054415226\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07429447025060654\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12350791692733765\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07315602153539658\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1431373506784439\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09876657277345657\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03399164602160454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018552714958786964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06429915875196457\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15337371826171875\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09997890889644623\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025730224326252937\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028441714122891426\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04429208114743233\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10616829246282578\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.309799462556839\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02444431185722351\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02410326711833477\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049001891165971756\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06280176341533661\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028014859184622765\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07046826183795929\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11398586630821228\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10008840262889862\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027278587222099304\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026089245453476906\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16302049160003662\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06101762875914574\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05856696888804436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07253749668598175\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16245625913143158\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07513794302940369\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03454146161675453\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12924516201019287\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030034353956580162\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048958152532577515\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020754888653755188\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05061006546020508\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07808596640825272\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06650844216346741\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07557004690170288\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.25463470816612244\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0038845217786729336\n",
      "Validation accuracy: 97.08\n",
      "\n",
      "Epoch 8..\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07934664189815521\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05525709316134453\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01297809835523367\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.12942150235176086\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15852247178554535\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04500151053071022\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0516611747443676\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046978846192359924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026888921856880188\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026369238272309303\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03691542148590088\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03382343426346779\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17343121767044067\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02122068777680397\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09084279090166092\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11226217448711395\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020126402378082275\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032008372247219086\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1301841139793396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09772619605064392\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13859359920024872\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06639086455106735\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050520796328783035\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02361445128917694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06602539122104645\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016085760667920113\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09439423680305481\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06028251349925995\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02969720959663391\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031915515661239624\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06359560042619705\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08298452198505402\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052779290825128555\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0239279605448246\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03838782384991646\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028536805883049965\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06400062143802643\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022186176851391792\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029280520975589752\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11288877576589584\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014535612426698208\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03840665891766548\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03164268657565117\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039724595844745636\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06127849221229553\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053474076092243195\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015068892389535904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06091751903295517\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11663085967302322\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013786884024739265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04777423292398453\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010267728008329868\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021964486688375473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06857442855834961\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07473303377628326\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021197941154241562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05139503628015518\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02296418510377407\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025828231126070023\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049311231821775436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03715240955352783\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036579594016075134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049683839082717896\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05252721160650253\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09145472943782806\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08921971917152405\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16215898096561432\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05155976489186287\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006117037497460842\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0583704337477684\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016840262338519096\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0317065455019474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03742038086056709\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10385593771934509\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05576404929161072\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014264038763940334\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0425746887922287\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08172571659088135\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016314348205924034\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0203204695135355\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021417519077658653\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09527726471424103\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12597167491912842\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07790956646203995\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021803975105285645\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03587183356285095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056640177965164185\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04811597988009453\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02669670060276985\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020506255328655243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06482042372226715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025426145642995834\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05128522962331772\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1392650306224823\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08995383977890015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05186925455927849\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011604811064898968\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010045859962701797\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10328016430139542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012710889801383018\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031060613691806793\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13024546205997467\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07321473211050034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11925472319126129\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04644323140382767\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01692088693380356\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0721769779920578\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027788057923316956\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04100213199853897\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009434561245143414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06423240154981613\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09351236373186111\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05572253838181496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03574768081307411\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.057843610644340515\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017658008262515068\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04728396236896515\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03760160878300667\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07170436531305313\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07554839551448822\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10761130601167679\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07433304935693741\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02737886644899845\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06134558469057083\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17362922430038452\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20454668998718262\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07049109041690826\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024519434198737144\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018642539158463478\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03296774625778198\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0195727851241827\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07616659253835678\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014989770948886871\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06874941289424896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09909448027610779\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13029532134532928\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11761760711669922\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05599803104996681\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034493621438741684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03934793546795845\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0342208556830883\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12187439948320389\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037870556116104126\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018251502886414528\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08572357892990112\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048428937792778015\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0911000519990921\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09602070599794388\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0771339014172554\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030724141746759415\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06590162217617035\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08798409253358841\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04748240485787392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011202364228665829\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048729121685028076\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01258728839457035\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055173955857753754\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12236104160547256\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12237177044153214\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06040769815444946\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14237573742866516\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04955452308058739\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03257184848189354\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0387410968542099\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07012180984020233\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024394072592258453\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10172907263040543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0709354355931282\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039735570549964905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07615732401609421\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024436116218566895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06007174775004387\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05365591496229172\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034878551959991455\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1112334132194519\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07063396275043488\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0600525327026844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0744423046708107\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14918947219848633\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03045245073735714\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.060516759753227234\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14438496530056\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08344350755214691\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04519498720765114\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022819828242063522\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03806589916348457\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04507485032081604\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035005006939172745\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020946426317095757\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0360422283411026\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04200298339128494\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17316439747810364\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024458445608615875\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10381618142127991\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025109771639108658\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09617684036493301\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05233805626630783\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10163936018943787\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15428587794303894\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09516377747058868\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045098867267370224\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13153813779354095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05437949672341347\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027704386040568352\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06330978125333786\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09121651202440262\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08870179206132889\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06500270217657089\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036400411278009415\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1304411143064499\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020602459087967873\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0408487506210804\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04934675619006157\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03339390829205513\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.151439368724823\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05741295963525772\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03362478315830231\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03481077030301094\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08661773055791855\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022572964429855347\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06476733088493347\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03636757656931877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062402330338954926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10218943655490875\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13620468974113464\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057138122618198395\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03966836631298065\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07736466079950333\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07519062608480453\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020812787115573883\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08529152721166611\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06670220196247101\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010569863021373749\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08939100801944733\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04140429571270943\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02846483327448368\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13903889060020447\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04710348695516586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058717940002679825\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04526760056614876\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012987403199076653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07324463874101639\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09089816361665726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03545958921313286\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11147011071443558\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08439046144485474\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018834061920642853\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041602972894907\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01210583746433258\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.060941897332668304\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08195698261260986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01661212183535099\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05522163584828377\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07673027366399765\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06963526457548141\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12802065908908844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04965865984559059\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02249177172780037\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03054690733551979\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03308974951505661\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03603233024477959\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020572490990161896\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03686438873410225\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004721108358353376\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02255506068468094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018330298364162445\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12717242538928986\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03706834092736244\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1262783259153366\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08148056268692017\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05378665030002594\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016789531335234642\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052382711321115494\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1432207077741623\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04915609210729599\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04104573279619217\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13561029732227325\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07092337310314178\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12577731907367706\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03846638649702072\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1426962912082672\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08234166353940964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08648595213890076\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1412254124879837\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0359865240752697\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05735553056001663\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05889975652098656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0992112085223198\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045148659497499466\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048039671033620834\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09450690448284149\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0194951593875885\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07633092999458313\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.10718590021133423\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05872077867388725\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0271709393709898\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061443161219358444\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14940370619297028\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028271352872252464\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013853147625923157\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0406365692615509\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014722781255841255\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0686146542429924\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2138550728559494\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09373949468135834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027393706142902374\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.119093157351017\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09503398835659027\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.061405882239341736\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02030795067548752\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052385468035936356\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.051570579409599304\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.046037059277296066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04497663304209709\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023282036185264587\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00841985922306776\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06810068339109421\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04888975992798805\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032685913145542145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07262646406888962\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0818270817399025\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029012253507971764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026038285344839096\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03100249171257019\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05491558834910393\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0691872239112854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05464150756597519\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11342206597328186\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053882889449596405\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03140959516167641\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14587965607643127\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05846446007490158\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11061409115791321\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15294206142425537\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11238308995962143\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013666030950844288\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06200479343533516\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03788754716515541\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024081120267510414\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056503962725400925\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006012349389493465\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1419658064842224\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026577839627861977\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020936762914061546\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.17525318264961243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1806805580854416\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027245931327342987\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04434046894311905\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03185810521245003\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039927538484334946\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03938072919845581\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041469186544418335\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06619468331336975\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06462205946445465\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07562478631734848\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026656029745936394\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0477515272796154\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06248817220330238\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11783085763454437\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013778556138277054\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05731426179409027\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03397728130221367\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03403094410896301\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10978863388299942\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0353858657181263\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08346667140722275\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07715905457735062\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07522277534008026\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07982338219881058\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041688501834869385\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05491628125309944\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028799153864383698\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01874503493309021\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1525944322347641\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07726041227579117\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09666755050420761\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0417247898876667\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07440129667520523\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04067424684762955\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19046932458877563\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.041519176214933395\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022439220920205116\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.22730103135108948\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03872602432966232\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02058202400803566\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0840863361954689\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019292892888188362\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044786132872104645\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06841486692428589\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15524330735206604\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.18313412368297577\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0381355956196785\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012563951313495636\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07109837979078293\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06881295889616013\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018405525013804436\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0490441620349884\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05847340449690819\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0805569738149643\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055227745324373245\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09101135283708572\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019277790561318398\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07368576526641846\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11433716118335724\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16195325553417206\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030227553099393845\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022407185286283493\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04136678948998451\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08191030472517014\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04705866798758507\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04564632102847099\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018164698034524918\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029979616403579712\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12597067654132843\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006294784136116505\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15568916499614716\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05844447389245033\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12467970699071884\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06000871583819389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0354953370988369\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11185819655656815\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10890428721904755\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011585335247218609\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030656158924102783\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02449876442551613\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04948728159070015\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06706630438566208\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04764271527528763\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1143290102481842\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054075535386800766\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14671474695205688\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13988444209098816\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05265497788786888\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.057292357087135315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06470204144716263\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027399331331253052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03390001505613327\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024234523996710777\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09629973024129868\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03968995437026024\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033710673451423645\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.27348726987838745\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03435749188065529\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04416141286492348\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033042993396520615\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054121460765600204\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10188018530607224\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14769725501537323\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04570388048887253\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049381181597709656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05982709303498268\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10244328528642654\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05565476417541504\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08193013072013855\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037349503487348557\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03385816887021065\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03495210409164429\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06975711137056351\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13575701415538788\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08685636520385742\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06773924827575684\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05102388933300972\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.046282459050416946\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09244795888662338\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05962901562452316\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061859067529439926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.01805908977985382\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046457886695861816\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00987412128597498\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06095046550035477\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019563987851142883\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011330748908221722\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05686027556657791\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05656132102012634\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07125800848007202\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07309184223413467\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030033783987164497\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011457588523626328\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08534875512123108\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12223229557275772\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06324111670255661\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03018202632665634\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06859413534402847\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04052405431866646\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.15592627227306366\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03396672010421753\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05133497342467308\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021595075726509094\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08734054118394852\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052646126598119736\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05405038222670555\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03136299178004265\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14744602143764496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010792826302349567\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026740889996290207\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025005878880620003\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07061779499053955\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04940230771899223\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03550199419260025\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05003505200147629\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035890813916921616\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03772854432463646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031320635229349136\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09215334802865982\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04969794303178787\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10040541738271713\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05553881824016571\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02520579844713211\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047190360724925995\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01242396142333746\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06360050290822983\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06827803701162338\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0650564581155777\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05884222313761711\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01982257328927517\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04356773942708969\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010312378406524658\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027908917516469955\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02189207449555397\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04736209288239479\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09774358570575714\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06491294503211975\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043056949973106384\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024857908487319946\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04251113906502724\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02073967643082142\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08023122698068619\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036989834159612656\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11053445190191269\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014694266021251678\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.016952121630311012\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014693998731672764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09071256220340729\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0356864407658577\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015608317218720913\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032641202211380005\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02506210282444954\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06505007296800613\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08807232975959778\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12939941883087158\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06633607298135757\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009693236090242863\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02711232379078865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04482298716902733\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042754001915454865\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.2397594004869461\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02800177037715912\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05374602973461151\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017974650487303734\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01971026137471199\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02579091489315033\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014404146932065487\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07612771540880203\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02493923529982567\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03287960961461067\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02899825945496559\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07605843245983124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07068003714084625\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02549641951918602\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11144021898508072\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08886783570051193\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015018409118056297\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010435291565954685\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0666404664516449\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23705320060253143\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028028937056660652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.051815424114465714\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03859202563762665\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01820971816778183\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.062164656817913055\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05487188324332237\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03302498161792755\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05238714441657066\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08705918490886688\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034547049552202225\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04478844627737999\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01704106107354164\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016693362966179848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030206404626369476\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023638438433408737\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09571263194084167\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0713585838675499\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031845055520534515\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031876277178525925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11631428450345993\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060103606432676315\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.23205633461475372\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.060520902276039124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04951583221554756\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018906187266111374\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013959300704300404\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.20386430621147156\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026156600564718246\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07013951987028122\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05657746642827988\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09938395768404007\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07376343011856079\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039440516382455826\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08102305233478546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07726866006851196\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11405885219573975\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1060764268040657\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03578485921025276\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011650769971311092\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015353555791079998\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02527964673936367\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009838082827627659\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054365795105695724\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10207004845142365\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030080005526542664\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058026984333992004\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006098962854593992\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08706144243478775\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051234614104032516\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08046266436576843\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10806693136692047\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0549735352396965\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01756175421178341\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02715292200446129\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012933767400681973\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14825990796089172\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13526977598667145\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08138246089220047\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10726701468229294\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05927875638008118\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1623050719499588\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041086096316576004\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03639654815196991\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06183543801307678\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09307849407196045\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0055879815481603146\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0403786301612854\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07852091640233994\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04169643297791481\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018101828172802925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051113057881593704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04162329062819481\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10321508347988129\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05295156314969063\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01611628197133541\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017146481201052666\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08598297089338303\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0594337098300457\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09213841706514359\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.022035524249076843\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07394368946552277\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012450441718101501\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05934290215373039\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06270261853933334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033063989132642746\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030283097177743912\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011580045334994793\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058893583714962006\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06017155200242996\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02343633584678173\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012973546981811523\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11451300978660583\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060490433126688004\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00761736324056983\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01533448975533247\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.044391799718141556\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06484642624855042\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06656114757061005\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02765161730349064\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04418057203292847\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04924241080880165\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0445813424885273\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020993022248148918\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0176659245043993\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030186058953404427\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006754718720912933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08616412431001663\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008430625312030315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08146397024393082\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05432604253292084\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028247317299246788\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04587651044130325\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16702504456043243\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012586106546223164\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021182682365179062\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017211735248565674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08072932809591293\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09928716719150543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058012109249830246\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03940275311470032\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03435725346207619\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11448032408952713\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030728863552212715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07488366216421127\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04214499518275261\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03976453095674515\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07037097215652466\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020755775272846222\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08618985116481781\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10691666603088379\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04933205619454384\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008450886234641075\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0712766945362091\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08728471398353577\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06915690004825592\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006084899418056011\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06995948404073715\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046836577355861664\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.19585959613323212\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05214647203683853\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0708913803100586\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01159073505550623\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09245683252811432\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054523978382349014\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010461779311299324\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07181164622306824\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020115038380026817\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0160910002887249\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08568966388702393\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015397624112665653\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10927657037973404\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08894858509302139\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011412999592721462\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07162974029779434\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03896050527691841\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054718710482120514\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02313077263534069\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.044015947729349136\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06560106575489044\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0507323294878006\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01777195744216442\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17722609639167786\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.2912472188472748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017924044281244278\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012449897825717926\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.047411318868398666\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08488942682743073\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05826636403799057\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1875779628753662\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22188270092010498\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09533437341451645\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.052581265568733215\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07064654678106308\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2199634611606598\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033066365867853165\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005212281830608845\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06385096907615662\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053186383098363876\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011149907484650612\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10609553754329681\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02098388411104679\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04795777425169945\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06681688129901886\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07818368077278137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10739120841026306\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01747356727719307\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03619188815355301\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0569438710808754\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13508254289627075\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028623342514038086\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03157253935933113\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17016971111297607\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07315103709697723\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062162186950445175\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02599029429256916\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048917256295681\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03818640485405922\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013949982821941376\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09782631695270538\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03250400722026825\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02100374735891819\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06023110821843147\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02173469215631485\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0432099923491478\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022810105234384537\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0684744194149971\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05891373008489609\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07606545835733414\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025684531778097153\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10836075991392136\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0877365916967392\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0500938817858696\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10977556556463242\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017794203013181686\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.058461688458919525\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02296624146401882\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10942813754081726\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011854571290314198\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038022227585315704\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06022876501083374\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02018510177731514\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022435033693909645\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12107416242361069\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1083010584115982\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034780677407979965\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0880647674202919\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030592599883675575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07139014452695847\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06181617081165314\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06604783982038498\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011507460847496986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01677734963595867\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06159171834588051\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05365226790308952\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03544100373983383\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09439097344875336\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09787959605455399\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04625377431511879\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12016652524471283\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028959553688764572\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08726798743009567\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1724785417318344\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05812365561723709\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05140058696269989\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1761794239282608\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007828017696738243\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04696517065167427\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03133855015039444\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03182823210954666\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11006602644920349\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042100828140974045\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11396683007478714\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023863447830080986\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1372726559638977\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08531485497951508\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15815268456935883\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08665738999843597\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019223803654313087\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030440358445048332\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09263947606086731\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07187159359455109\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13239304721355438\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07564778625965118\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14165857434272766\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01128744799643755\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009000342339277267\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04864966496825218\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018982090055942535\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14700186252593994\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09085077047348022\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04536836966872215\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05931183695793152\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03861871361732483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05131957679986954\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043214257806539536\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08450727164745331\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06426417082548141\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.19145376980304718\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07168852537870407\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02421107515692711\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03984124958515167\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050974201411008835\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06402404606342316\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02920394204556942\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05243038013577461\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12202700227499008\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07109718024730682\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05727146565914154\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049529287964105606\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10141094028949738\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08507345616817474\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14515018463134766\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02795904129743576\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0720445066690445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02941180393099785\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.051096562296152115\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017647910863161087\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03706424683332443\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14656709134578705\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07880004495382309\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010723398067057133\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0733947604894638\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08054593950510025\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017964839935302734\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08580706268548965\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06608334183692932\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09126172214746475\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06332177668809891\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04278377816081047\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14912134408950806\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10308876633644104\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04777568578720093\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0688268318772316\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007911980152130127\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02594538778066635\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011997424997389317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11961193382740021\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12671132385730743\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06250806152820587\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03307618200778961\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12626324594020844\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022890014573931694\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07700596004724503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047597192227840424\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1375371813774109\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04250472038984299\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12773685157299042\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030144214630126953\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08269301056861877\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05029967427253723\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06773866713047028\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036414630711078644\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13077592849731445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.021728895604610443\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.023769475519657135\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0806344598531723\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03342698514461517\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04356631264090538\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03531712666153908\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07183951884508133\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016701621934771538\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03918207436800003\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02238084003329277\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0924861878156662\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040105611085891724\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09676336497068405\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03822469338774681\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04804383963346481\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03150026872754097\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11398164182901382\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07745907455682755\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05320224165916443\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02306988276541233\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06405972689390182\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02322041615843773\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11112895607948303\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1563524454832077\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017020566388964653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07099413871765137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05892083793878555\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014676542021334171\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05744297057390213\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.103973887860775\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02958119660615921\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12207560986280441\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12789660692214966\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.042824506759643555\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.044965654611587524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05023079738020897\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06915061920881271\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06372648477554321\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06300587952136993\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03802463784813881\n",
      "Validation accuracy: 97.43\n",
      "\n",
      "Epoch 9..\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015181031078100204\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06945831328630447\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013843892142176628\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07200586050748825\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03789905831217766\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02096526138484478\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025407860055565834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016585232689976692\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06842729449272156\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08806732296943665\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03433578461408615\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09435725212097168\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044973909854888916\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013678107410669327\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01446322351694107\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03314892575144768\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06958343088626862\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07470623403787613\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06816764920949936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0547226145863533\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017800653353333473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04867757111787796\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0580926276743412\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03156132251024246\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01989409141242504\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06337731331586838\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020952017977833748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027800368145108223\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03253822401165962\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044702161103487015\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04358076676726341\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043645333498716354\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010596521198749542\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050428394228219986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02935909479856491\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01007163804024458\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08294651657342911\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02510860562324524\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060129180550575256\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10187730193138123\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006244343239814043\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0119820861145854\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.048735395073890686\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06389223039150238\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026816055178642273\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029096372425556183\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015330500900745392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03867248073220253\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006830060854554176\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07434391975402832\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.050759028643369675\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0860157459974289\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0217288788408041\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05871199443936348\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013737506233155727\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.045347340404987335\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059310946613550186\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044347383081912994\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04557923972606659\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08261477202177048\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033714115619659424\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0219908207654953\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03620143234729767\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015832560136914253\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08042029291391373\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02218104712665081\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020616384223103523\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07723473757505417\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012752000242471695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10413262993097305\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09233969449996948\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01675672084093094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018583739176392555\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10149701684713364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025425642728805542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02786855399608612\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022096823900938034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08518677949905396\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037553273141384125\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032779060304164886\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06718987971544266\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042786262929439545\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016373658552765846\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07694283127784729\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02804887667298317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12073402106761932\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0322149433195591\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02280939184129238\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02764184959232807\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0342155396938324\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031138254329562187\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02557993307709694\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04559401422739029\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02419961802661419\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022083092480897903\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023806748911738396\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10295115411281586\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019655967131257057\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09026351571083069\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024132106453180313\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0884026288986206\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04604814574122429\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020418278872966766\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06371316313743591\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028320835903286934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0335773341357708\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032205335795879364\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01811918243765831\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0691278725862503\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015147850848734379\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1248450055718422\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04208742454648018\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0178638007491827\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03815067186951637\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039795394986867905\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06710676848888397\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03946305438876152\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016582565382122993\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06369289010763168\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01846075989305973\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15471017360687256\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012455377727746964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04042651876807213\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037948887795209885\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03205651789903641\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03477764502167702\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027515891939401627\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023860130459070206\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011315860785543919\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009938692674040794\n",
      "Training accuracy: 90.62\n",
      "Training loss: 0.1829272210597992\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08870241791009903\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.023462390527129173\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032515186816453934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04513181373476982\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08502859622240067\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06533221155405045\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02464785985648632\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0565795972943306\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015773972496390343\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02538551576435566\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11509627103805542\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09615664929151535\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02839166484773159\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015976104885339737\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017815716564655304\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07548343390226364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03695566579699516\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025092868134379387\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06887572258710861\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026359491050243378\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03474229946732521\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01924431510269642\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03916575014591217\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031098894774913788\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0044663045555353165\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04123440757393837\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.048960793763399124\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09540462493896484\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015559292398393154\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11782276630401611\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0940057784318924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007684784475713968\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04032934457063675\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029138801619410515\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.20341242849826813\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10841214656829834\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08555397391319275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0799427404999733\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0363713875412941\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02917580120265484\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02679155021905899\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03006621077656746\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06239861249923706\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05518646165728569\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004866432398557663\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07112712413072586\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021309267729520798\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06359811127185822\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10467350482940674\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.052902448922395706\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.044450365006923676\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013473756611347198\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0388316810131073\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07288522273302078\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0388050340116024\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10852400213479996\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.14790908992290497\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05768177658319473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07451175153255463\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02241561934351921\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010809161700308323\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029187597334384918\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03005382977426052\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034170132130384445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0848151296377182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08400527387857437\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09666939079761505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019185619428753853\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02551201917231083\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11956088244915009\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037797898054122925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05434538051486015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06759563833475113\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09348571300506592\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.19693055748939514\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0650121197104454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021998029202222824\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02050996944308281\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03504151850938797\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044263262301683426\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04131792113184929\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04313255473971367\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025100216269493103\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02098056674003601\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012384600006043911\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07110175490379333\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05975067988038063\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05304092913866043\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0286663006991148\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024304479360580444\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047815270721912384\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019294966012239456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03391500562429428\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025621797889471054\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06383813917636871\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037068407982587814\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02560158260166645\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022393837571144104\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12411870062351227\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05128150060772896\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05863432586193085\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01572677120566368\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07340091466903687\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.046392280608415604\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03103523701429367\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03145688399672508\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025958584621548653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.022316474467515945\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08019427955150604\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10100263357162476\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08246748894453049\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08448348194360733\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010093930177390575\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11804644018411636\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.178352490067482\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04061718285083771\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05329709127545357\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024219684302806854\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026469683274626732\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016601433977484703\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07704746723175049\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.020316703245043755\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16040106117725372\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1104602962732315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048269908875226974\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028743747621774673\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05573322996497154\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048980168998241425\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06436049938201904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06785790622234344\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011921370401978493\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03760499507188797\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030756890773773193\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02411939576268196\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0819927453994751\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06696957349777222\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019645294174551964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049941763281822205\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10091349482536316\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14717599749565125\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008482498116791248\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10322386771440506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10443577170372009\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06997337937355042\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04196303337812424\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015455376356840134\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07200709730386734\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04272715002298355\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037499330937862396\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06467586010694504\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09297887235879898\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05881643295288086\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09918023645877838\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05229580029845238\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02268890291452408\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034094687551259995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05578474700450897\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02803511917591095\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.061139367520809174\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017296843230724335\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0372302383184433\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027339546009898186\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09396054595708847\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10514061152935028\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013294024392962456\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005203558132052422\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1088324785232544\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03724665194749832\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12913481891155243\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019194981083273888\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02281198464334011\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04451310262084007\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020751699805259705\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06530781835317612\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028612613677978516\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030858337879180908\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04232005402445793\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0190337635576725\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05185775086283684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06980814039707184\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0558588020503521\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014322981238365173\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.063851498067379\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029093118384480476\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05822897329926491\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05666092410683632\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028181830421090126\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029280666261911392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007619349751621485\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01968790963292122\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07755639404058456\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018897077068686485\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0035130339674651623\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1823016107082367\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02049202099442482\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0630122646689415\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06331333518028259\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01756620593369007\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07083745300769806\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007203367073088884\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032559435814619064\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0435074083507061\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06655212491750717\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01825505495071411\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06273394823074341\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0888141617178917\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05473409220576286\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03378994017839432\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020135117694735527\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0744967833161354\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05826226994395256\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010744934901595116\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023904990404844284\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04915783554315567\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04665233567357063\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048983704298734665\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037875693291425705\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0479779951274395\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07448779791593552\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02639518678188324\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16306287050247192\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05627564340829849\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035216979682445526\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07513970881700516\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0653228908777237\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027017980813980103\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06441938132047653\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0564473457634449\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04270824417471886\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.049437832087278366\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04722252115607262\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.035337984561920166\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05165216326713562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0500108003616333\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08879519999027252\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028967127203941345\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0226239375770092\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035932160913944244\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.054466281086206436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06828813254833221\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0312936007976532\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031732238829135895\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056620966643095016\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042337335646152496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005769182927906513\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01669119857251644\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08348698914051056\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053271155804395676\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017216864973306656\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02041768841445446\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05237804725766182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06515941768884659\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03158196434378624\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039559297263622284\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012592760846018791\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017294839024543762\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07262338697910309\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03516588360071182\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10434117913246155\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05231327563524246\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1535670906305313\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12296058237552643\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008232969790697098\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02143862284719944\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12889693677425385\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03536750748753548\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017541110515594482\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10075315833091736\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027865244075655937\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060495033860206604\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013136452063918114\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06996884196996689\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05456900596618652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12890881299972534\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06018663942813873\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028600871562957764\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08224797993898392\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04035837948322296\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04365575313568115\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014172027818858624\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028624307364225388\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014317657798528671\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04080874100327492\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012503817677497864\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04888087883591652\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027468794956803322\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0317627415060997\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07081010937690735\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07366277277469635\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011509611271321774\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009251813404262066\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00946769118309021\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09689164906740189\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12790389358997345\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0544818751513958\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0348387248814106\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020316816866397858\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029993519186973572\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02699686959385872\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03527606651186943\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041973721235990524\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02318028174340725\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09083957225084305\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05036060884594917\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043027348816394806\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037254758179187775\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11223866790533066\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04971533268690109\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05124516040086746\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12851426005363464\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10102301836013794\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05905555188655853\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008512904867529869\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006286610383540392\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06433720886707306\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09149888157844543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08918524533510208\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021437181159853935\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02255689911544323\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14055053889751434\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059151388704776764\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02073758840560913\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03842180594801903\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007626743987202644\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03989597409963608\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05560743808746338\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037396762520074844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033729150891304016\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18904243409633636\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021334215998649597\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0372813418507576\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03639037162065506\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.047675881534814835\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014213154092431068\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08824774622917175\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10410148650407791\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13439500331878662\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09854777157306671\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07654795795679092\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01627788133919239\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014949871227145195\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0415429063141346\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06759224086999893\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0532839410007\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15720322728157043\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017690371721982956\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02214321866631508\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03182636573910713\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015276676043868065\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04429524019360542\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.15310990810394287\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005079467780888081\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05214478075504303\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026599852368235588\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038919832557439804\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061914797872304916\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.015491116791963577\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06888334453105927\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.14675413072109222\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04712914302945137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04455803334712982\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014679556712508202\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053906120359897614\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.034516870975494385\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.16337624192237854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12222893536090851\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032591432332992554\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014771852642297745\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041542600840330124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03329021856188774\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019917557016015053\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028341354802250862\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07599671185016632\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06958159804344177\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10781063884496689\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12275402992963791\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025169648230075836\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02734130620956421\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08831750601530075\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07263264805078506\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08571585267782211\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08017773181200027\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013129149563610554\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03433378413319588\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022698787972331047\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05451223626732826\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.045330438762903214\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0998014435172081\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15259607136249542\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04282847046852112\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05972892791032791\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06797126680612564\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06755118817090988\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09095755964517593\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.022460617125034332\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06983856111764908\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06099065765738487\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04597457870841026\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12024296820163727\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08670690655708313\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030272504314780235\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029671965166926384\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017664073035120964\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06109923496842384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12954957783222198\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0452178530395031\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10220488905906677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0642702504992485\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05699713155627251\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06134805083274841\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01775679923593998\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0586492083966732\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032879579812288284\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08891993016004562\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08602182567119598\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04813893511891365\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0236199963837862\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030043870210647583\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010665790177881718\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05708209425210953\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04244136065244675\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02887715771794319\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1458975374698639\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07250110805034637\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.020339317619800568\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09432689100503922\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020283766090869904\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.051336150616407394\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02614818513393402\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018444977700710297\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11253486573696136\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12392735481262207\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03866147622466087\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03760542720556259\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03402207791805267\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1007920652627945\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05754268169403076\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0593780092895031\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0958600640296936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04263433441519737\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02106918953359127\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05213993042707443\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040314964950084686\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05587099865078926\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12362325936555862\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015088588930666447\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03168565407395363\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021155821159482002\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11171461641788483\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022507119923830032\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013167922385036945\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.041869375854730606\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1750885546207428\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010458927601575851\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02234111912548542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013990482315421104\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048995453864336014\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011608988046646118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038229163736104965\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04257875308394432\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0190951656550169\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.062317751348018646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07717388868331909\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025199394673109055\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056901901960372925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038344454020261765\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04708130285143852\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0660196840763092\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011836652643978596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04737536236643791\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017933616414666176\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015303120017051697\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035510119050741196\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026334566995501518\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06463439762592316\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.059877630323171616\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057990267872810364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0344930961728096\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08988304436206818\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0792718231678009\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028516052290797234\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04382271319627762\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057206977158784866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01794924959540367\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0345594547688961\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04735725373029709\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10242784023284912\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07564415782690048\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07530899345874786\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022316541522741318\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02690211683511734\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.22764940559864044\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10159920156002045\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02286183275282383\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06346874684095383\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10711117088794708\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08406910300254822\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11977479606866837\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02016136236488819\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03785092383623123\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03598140552639961\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09695524722337723\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1320408284664154\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03707846626639366\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05569346249103546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12925134599208832\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012232312001287937\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05781151354312897\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09888481348752975\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04699068143963814\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0703132301568985\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09774643927812576\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012707365676760674\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02204231731593609\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027674533426761627\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04446849599480629\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060817040503025055\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01854827255010605\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02598627470433712\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03894735872745514\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015618841163814068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0150655098259449\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031223446130752563\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029908116906881332\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011631511151790619\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19122466444969177\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1697283238172531\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021490395069122314\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04159918054938316\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016484152525663376\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01596931740641594\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12270056456327438\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05155343562364578\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09314211457967758\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026231564581394196\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027973109856247902\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10646615922451019\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023779259994626045\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13326694071292877\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02829815447330475\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07386334985494614\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01579691842198372\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.045208640396595\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03919867426156998\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04946868494153023\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06737107038497925\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.055546022951602936\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04479102045297623\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01256816927343607\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08829869329929352\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05086134001612663\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02265014685690403\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05654885619878769\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03623127192258835\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05359380319714546\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03353504091501236\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07559819519519806\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11598752439022064\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0398709736764431\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04992479458451271\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029915453866124153\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09800243377685547\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011378035880625248\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018747245892882347\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09296216815710068\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0352511964738369\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07214505225419998\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015181007795035839\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05766463652253151\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020850105211138725\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08804567158222198\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00991269201040268\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061027318239212036\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09831809997558594\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03136412054300308\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028706954792141914\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03541148081421852\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030733536928892136\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02696007303893566\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11651188880205154\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03815925121307373\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09356100857257843\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10218389332294464\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05693726986646652\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10096786171197891\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08133799582719803\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08623525500297546\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09180504828691483\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09801021963357925\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09655631333589554\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15099267661571503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05652567371726036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08453421294689178\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09048785269260406\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15248285233974457\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05334338918328285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09405863285064697\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08964419364929199\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038632363080978394\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11303617805242538\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02780982479453087\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022070201113820076\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037841565907001495\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05646840110421181\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05375930294394493\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033360544592142105\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.18924085795879364\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013321267440915108\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13041315972805023\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03746119141578674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05943015217781067\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04668176546692848\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07145526260137558\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.031148143112659454\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05703459680080414\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025675466284155846\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008949825540184975\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03841676190495491\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02803877927362919\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05608789622783661\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01294289156794548\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02962649241089821\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056479111313819885\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025000588968396187\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03208944946527481\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10528293251991272\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13539370894432068\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036514025181531906\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01283111609518528\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08391278982162476\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009939230978488922\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.11057911068201065\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026280267164111137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03651051223278046\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12129761278629303\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03265533596277237\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007599721662700176\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.039912473410367966\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14337508380413055\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036500200629234314\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.043814174830913544\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07214725017547607\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.043869130313396454\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04301130399107933\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11940483003854752\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04185226932168007\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018559319898486137\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024121243506669998\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03788848966360092\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025862744078040123\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07522963732481003\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006083211395889521\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017037948593497276\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06249093636870384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1017560213804245\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009213930927217007\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02592971920967102\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01605152152478695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06436330080032349\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022078873589634895\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02057351917028427\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.039902396500110626\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03246982768177986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029497405514121056\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06256325542926788\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033063530921936035\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05838227644562721\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0761481449007988\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1051512360572815\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04364122822880745\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04584353044629097\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01979137398302555\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02638559229671955\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029054978862404823\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016939930617809296\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06992831826210022\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029531927779316902\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014125673100352287\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01720305159687996\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0067334589548408985\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04293873906135559\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033369988203048706\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049786053597927094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02809993550181389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03502359241247177\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05390213429927826\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05675780028104782\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.048676714301109314\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12684893608093262\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07844996452331543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04844968393445015\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011215325444936752\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08027675002813339\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009919285774230957\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026223614811897278\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04712182655930519\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015328006818890572\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03640136867761612\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025022272020578384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08865030109882355\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019433630630373955\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0625385195016861\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012988107278943062\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14666664600372314\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03481096774339676\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09373626857995987\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12219206988811493\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08186208456754684\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06254192441701889\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009416312910616398\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04706065356731415\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024763232097029686\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0764964148402214\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034858912229537964\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029475610703229904\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12969908118247986\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047104526311159134\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026610303670167923\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12946535646915436\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06295579671859741\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.15861956775188446\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06113547831773758\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08465314656496048\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032760053873062134\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08097436279058456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.023333081975579262\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04548010230064392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03101157769560814\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05151510611176491\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014381157234311104\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10642866045236588\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04743083938956261\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044485755264759064\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14651019871234894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05471675470471382\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01449383981525898\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06100066751241684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05162467062473297\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020756920799613\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05121637135744095\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10511002689599991\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02433675527572632\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0330558605492115\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04225219413638115\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033377423882484436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10864903032779694\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014831104315817356\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07051528245210648\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.16783356666564941\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021430520340800285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09117437154054642\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12056506425142288\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016342194750905037\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.082383893430233\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10720334947109222\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030007170513272285\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028519947081804276\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04070050269365311\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012426165863871574\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17128640413284302\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04440431296825409\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09011925756931305\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05278785154223442\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04722583293914795\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13363604247570038\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02858259342610836\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03756053373217583\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04055400937795639\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007166936062276363\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07465921342372894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058970652520656586\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030995821580290794\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039427731186151505\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027071092277765274\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10109361261129379\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013589320704340935\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.09940394014120102\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0550040528178215\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011682120151817799\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11792067438364029\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0072827450931072235\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02299703098833561\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0726524293422699\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03037724643945694\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01988965831696987\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006806419230997562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05356425791978836\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08092494308948517\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09748055785894394\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01201501116156578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08345158398151398\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03145889192819595\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.21003276109695435\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13718117773532867\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03059033304452896\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055823247879743576\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04143107309937477\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05236561968922615\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010179736651480198\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.2965908646583557\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09583498537540436\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049606192857027054\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006825841031968594\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042913708835840225\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012391617521643639\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08103832602500916\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04178732633590698\n",
      "Validation accuracy: 97.39\n",
      "\n",
      "Epoch 10..\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032830026000738144\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032518837600946426\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0851101204752922\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011674354784190655\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07199966162443161\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08999820053577423\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0910421833395958\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015717322006821632\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04212494194507599\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025911377742886543\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05465373396873474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04093636944890022\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03837008774280548\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07092447578907013\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0354459285736084\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03165539354085922\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07672371715307236\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09717202186584473\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07841408997774124\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01966211386024952\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04401993006467819\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0883619636297226\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06759919971227646\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048300135880708694\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.038863420486450195\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014431910589337349\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004899125080555677\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05570734664797783\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.08751675486564636\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030090242624282837\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029520902782678604\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03911997377872467\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02081717923283577\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026658419519662857\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05719514563679695\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007025350816547871\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03656989336013794\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034936320036649704\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12780597805976868\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09132262319326401\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12143925577402115\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05617605894804001\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010583803988993168\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08794274181127548\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0330314077436924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012138749472796917\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0165267251431942\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024881215766072273\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.035292528569698334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04392021894454956\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027866747230291367\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10506801307201385\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01848486438393593\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021080827340483665\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12193602323532104\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014642884023487568\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019497107714414597\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05397463217377663\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036632705479860306\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06918564438819885\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03693576529622078\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0507764033973217\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07504042237997055\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004337261896580458\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03282277658581734\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.06112316623330116\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10039656609296799\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023970721289515495\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03600781038403511\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10947449505329132\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034835830330848694\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049722570925951004\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018100300803780556\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017419015988707542\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027444902807474136\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12467101216316223\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02398492768406868\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11231910437345505\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0367758683860302\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012485416606068611\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03116614744067192\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07131682336330414\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04926156625151634\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022204196080565453\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02665506862103939\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04105529189109802\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02700762264430523\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07165221124887466\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04780598357319832\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01916743628680706\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09938322752714157\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03534248471260071\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09081315249204636\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012785748578608036\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04296450689435005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06769204884767532\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011928982101380825\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024912910535931587\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.01942220889031887\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04257771372795105\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0295355636626482\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008645342662930489\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0730830505490303\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017819421365857124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031028760597109795\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06254846602678299\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10165274888277054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028244834393262863\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030987510457634926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03462113440036774\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0456635020673275\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.059096988290548325\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037586845457553864\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024341154843568802\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12219250947237015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05401007458567619\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013072289526462555\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03261885419487953\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.003269795561209321\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05798639357089996\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01565544307231903\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013649895787239075\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019593117758631706\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11153531819581985\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024741776287555695\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018406089395284653\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017905645072460175\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008640077896416187\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017437418922781944\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03408996760845184\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01200844906270504\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11719094216823578\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09010250866413116\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01027936302125454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07767262309789658\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03223495930433273\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0316312238574028\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02919691428542137\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039303213357925415\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04777253791689873\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012203743681311607\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009443763643503189\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03555549308657646\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.044514499604701996\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15247733891010284\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0450056754052639\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04698736593127251\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07230841368436813\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09434744715690613\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0243520624935627\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01948467083275318\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029008496552705765\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07916539907455444\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05977647751569748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019112616777420044\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024786505848169327\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05524580925703049\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01294758077710867\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.022609448060393333\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021064074710011482\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01082448661327362\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02850147895514965\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013774903491139412\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04339516535401344\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016455817967653275\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05094785988330841\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02726450003683567\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11214569211006165\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08595696091651917\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014531934633851051\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06712625920772552\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02843351475894451\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1122768223285675\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04173325002193451\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014020255766808987\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07166538387537003\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014788744039833546\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01275759469717741\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04739097133278847\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029082652181386948\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02774788625538349\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055008482187986374\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07705418020486832\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04913008585572243\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016540316864848137\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023834656924009323\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0833616554737091\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015263834036886692\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044020287692546844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045952100306749344\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023290172219276428\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05897193029522896\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053860124200582504\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014669440686702728\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09913729131221771\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10976941883563995\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03506280109286308\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.058733969926834106\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028742386028170586\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08022954314947128\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05308084189891815\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018540531396865845\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011606398969888687\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008458631113171577\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07821248471736908\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04380982369184494\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17094950377941132\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10936511307954788\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01432142872363329\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.061627838760614395\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013816015794873238\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06046946346759796\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06032802164554596\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08610296994447708\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025545353069901466\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016635434702038765\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1355699896812439\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0734984278678894\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04249582067131996\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0294832494109869\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07980774343013763\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10060151666402817\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0418722927570343\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1342451274394989\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01915345899760723\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03647411987185478\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.033043429255485535\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04641884192824364\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.055106714367866516\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10843372344970703\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.048913829028606415\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016231456771492958\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02149765007197857\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04060336574912071\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01713518053293228\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059755343943834305\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05964624136686325\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037979550659656525\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22821569442749023\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.043499503284692764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.040584269911050797\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052994951605796814\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020910056307911873\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024037186056375504\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.044948965311050415\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.019900575280189514\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0570223405957222\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041804809123277664\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0982431098818779\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03440339118242264\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07964660972356796\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036808717995882034\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05748744681477547\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025117700919508934\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0954151451587677\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1450401246547699\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03407873213291168\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.16186951100826263\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13775116205215454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026276761665940285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05460735410451889\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017635522410273552\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07249180227518082\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0344574935734272\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.032103024423122406\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035335250198841095\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02650947868824005\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028566977009177208\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.22885431349277496\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02969423308968544\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07774335145950317\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007964679040014744\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02356072887778282\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018794113770127296\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02337764762341976\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011510493233799934\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08416145294904709\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03010658733546734\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028081733733415604\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016081875190138817\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02357066608965397\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033928148448467255\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04468425735831261\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05416201055049896\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026051659137010574\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05120139941573143\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.060661427676677704\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0424373634159565\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06929371505975723\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08498439937829971\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02179192192852497\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030369212850928307\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09422571212053299\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04648280143737793\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03841594606637955\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01640641875565052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03380957990884781\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018638990819454193\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04085533693432808\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037195052951574326\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04293016344308853\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010717591270804405\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056023009121418\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03996293246746063\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0375085175037384\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.17474642395973206\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03220859915018082\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05418800562620163\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.26683497428894043\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03311285004019737\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020542269572615623\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010026780888438225\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01293773390352726\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06513287127017975\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05618360638618469\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04177231714129448\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03769627586007118\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037841543555259705\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0747307687997818\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01640436239540577\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022278960794210434\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08166683465242386\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06892824918031693\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0709073543548584\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.052042439579963684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05621882155537605\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09212668985128403\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042754240334033966\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023865872994065285\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019109798595309258\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03741947561502457\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.034579891711473465\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04258459433913231\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028607362881302834\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028389355167746544\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03865281492471695\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08249473571777344\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007058192044496536\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08673615753650665\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09242641180753708\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03350921720266342\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03934497386217117\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04034671187400818\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.12250084429979324\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0570342019200325\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011477416381239891\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012448901310563087\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010843247175216675\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12413505464792252\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030551621690392494\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0496140755712986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019781485199928284\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04541778564453125\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0173041932284832\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.060694046318531036\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011956572532653809\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0400393009185791\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03201575577259064\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017529333010315895\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.016818007454276085\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020872335880994797\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04834531992673874\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016097789630293846\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010382547043263912\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.052552830427885056\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030757756903767586\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039635639637708664\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05892958492040634\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.057017259299755096\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024418115615844727\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10274278372526169\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04814682900905609\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020220080390572548\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09511993825435638\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06474920362234116\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11193795502185822\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07502538710832596\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026074230670928955\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03577638417482376\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022515298798680305\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.059689123183488846\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008788876235485077\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.1133379265666008\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.036021795123815536\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04551416262984276\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05510842800140381\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08994535356760025\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02222929336130619\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008823825977742672\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.020057085901498795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027234943583607674\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044578418135643005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.035186517983675\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06087654083967209\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037318866699934006\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.037257663905620575\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.03837865591049194\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08755799382925034\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02734239585697651\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042604632675647736\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0852271020412445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.026385389268398285\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05831289663910866\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010114093311131\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004604038316756487\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.053383368998765945\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07834690809249878\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04436158761382103\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04093293100595474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03639175370335579\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014306087046861649\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10770592093467712\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033477187156677246\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06495505571365356\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09974154084920883\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007023677229881287\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02424371987581253\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03325442224740982\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01819922961294651\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03759489953517914\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016399521380662918\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043801892548799515\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011976087465882301\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03049984946846962\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05013221502304077\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01948244869709015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033395037055015564\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.11818347871303558\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030518030747771263\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.11081476509571075\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06235458329319954\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018766289576888084\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06514283269643784\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.047793712466955185\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.053115103393793106\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02118406444787979\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1116018146276474\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038513269275426865\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03239881619811058\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05738792568445206\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09193626046180725\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04094550386071205\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0124840522184968\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06922408938407898\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03624524921178818\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03446093201637268\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014755851589143276\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03722023218870163\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09528110176324844\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013383831828832626\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05145162716507912\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07137349992990494\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012555267661809921\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03647223114967346\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.053971465677022934\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06576121598482132\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028062691912055016\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005589049309492111\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016377510502934456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02397654950618744\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.024847662076354027\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017111143097281456\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044230926781892776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04940576106309891\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03297000378370285\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03768205642700195\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007850544527173042\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01267458125948906\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014531678520143032\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06709475070238113\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08555399626493454\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04316273704171181\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024429112672805786\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03661971539258957\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.1370842456817627\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05273685231804848\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02912876009941101\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08620036393404007\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.16465751826763153\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.041881706565618515\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12812867760658264\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06549578905105591\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038205139338970184\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030265377834439278\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.056387972086668015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04311012476682663\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06373985856771469\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04947797209024429\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04325293377041817\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026480358093976974\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.15261726081371307\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09628405421972275\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03817356750369072\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02503441832959652\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.058361493051052094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012425722554326057\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04613368213176727\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02115090750157833\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02342415042221546\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021232618018984795\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026675468310713768\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04826468229293823\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06973899900913239\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03169965744018555\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06317173689603806\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024587251245975494\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04811403527855873\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01392627228051424\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01771623268723488\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09205751121044159\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012378540821373463\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08702553808689117\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09478432685136795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07422331720590591\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08639133721590042\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.040979016572237015\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.039872393012046814\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011186392977833748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01454133540391922\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029113804921507835\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06323271244764328\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0816650465130806\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023300450295209885\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05043719708919525\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05114500969648361\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025180980563163757\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03979693725705147\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04217924922704697\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09930205345153809\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02810709737241268\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02338363602757454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05391405522823334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05809946730732918\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03904171288013458\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015955783426761627\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008632028475403786\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05060756206512451\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09711813181638718\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.14473704993724823\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09307259321212769\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02799576334655285\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01737799309194088\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016051530838012695\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09257286787033081\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02634820155799389\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04370928183197975\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02603510580956936\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08029283583164215\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03725988045334816\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06041519716382027\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.046686988323926926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.049582432955503464\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10977033525705338\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03108074702322483\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06402852386236191\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023644497618079185\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11475933343172073\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016811562702059746\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023697562515735626\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05391347035765648\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08348486572504044\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030006729066371918\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01613030768930912\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03412943705916405\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03384022414684296\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.10556702315807343\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07789020985364914\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010823916643857956\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02411099709570408\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00799737498164177\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07702943682670593\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01926283910870552\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.020487960427999496\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06163123622536659\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10249531269073486\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04965272545814514\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010494226589798927\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016716502606868744\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03909188508987427\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03859495744109154\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021083146333694458\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03565744310617447\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01893709972500801\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04060518369078636\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.167832612991333\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016942694783210754\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033423587679862976\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030535705387592316\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010987021960318089\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0820951759815216\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025634022429585457\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09472420066595078\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05625573545694351\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029326580464839935\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010631181299686432\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06501452624797821\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09601269662380219\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026554014533758163\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031693924218416214\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.020561430603265762\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.050676729530096054\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03399549797177315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07472209632396698\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.038723837584257126\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05926474556326866\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05541159585118294\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04871949180960655\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05368797853589058\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05348322167992592\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012033659964799881\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054050300270318985\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07596928626298904\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03236224502325058\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021106524392962456\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0566951185464859\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06510358303785324\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016977760940790176\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.042791325598955154\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014917606487870216\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018956443294882774\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03917567431926727\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05526572838425636\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01586979627609253\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03814735263586044\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017460092902183533\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014413201250135899\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0136526208370924\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014575309120118618\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03069714829325676\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01638174057006836\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07940522581338882\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.122678242623806\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.12690533697605133\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01878901571035385\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0553310252726078\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015468545258045197\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06755759567022324\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03564498946070671\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022249631583690643\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04982226341962814\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04046710580587387\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04241311550140381\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005125468596816063\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0537744015455246\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03157717362046242\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1286163032054901\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018248412758111954\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009008573368191719\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014876283705234528\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005437870975583792\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08465267717838287\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0389612540602684\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07623077929019928\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10880699753761292\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00923193246126175\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07267411053180695\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009838297963142395\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04733789339661598\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01531003974378109\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009801583364605904\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029954269528388977\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07614096254110336\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009287647902965546\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.025744006037712097\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03577999025583267\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029336588457226753\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0880700945854187\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02150038629770279\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029454488307237625\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030537866055965424\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03174205124378204\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02257133089005947\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.05048900470137596\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011515596881508827\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018892036750912666\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1250886768102646\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04142746329307556\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05196703225374222\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.09197758138179779\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012646631337702274\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02511928603053093\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08787287771701813\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04739987477660179\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005197964143007994\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07160565257072449\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014576763845980167\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04263848438858986\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0049309260211884975\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07084126770496368\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.1335192769765854\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06257613003253937\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04741949588060379\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.18917638063430786\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07693002372980118\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04747788608074188\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.00888328067958355\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0030734091997146606\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07812076807022095\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0046486081555485725\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007359472569078207\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018125446513295174\n",
      "Training accuracy: 92.19\n",
      "Training loss: 0.1977701485157013\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008359000086784363\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06685847789049149\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012616230174899101\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03715566173195839\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05887966230511665\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02826692909002304\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014096062630414963\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04677877202630043\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.051604148000478745\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05153609439730644\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013562412932515144\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09835110604763031\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017408933490514755\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.006744230166077614\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038830507546663284\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02954499050974846\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05555577948689461\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02205287106335163\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0399114191532135\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.008108296431601048\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02852143719792366\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02731667459011078\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005453184247016907\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04228058084845543\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10646424442529678\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03776431456208229\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.17097268998622894\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06273800879716873\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.043081775307655334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03686211258172989\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.019085247069597244\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.012823755852878094\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01745612919330597\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.026429004967212677\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0629347711801529\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10073106735944748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.036137502640485764\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04743979871273041\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01113268081098795\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.054678115993738174\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.18094263970851898\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03171252831816673\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013277911581099033\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0734318345785141\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01060903538018465\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022895900532603264\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05548068508505821\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.1162499189376831\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.017170557752251625\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.05379210039973259\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.10283498466014862\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.055388592183589935\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05796217918395996\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025100577622652054\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0854705199599266\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05734589695930481\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014135956764221191\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01060001365840435\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.056247301399707794\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04855003207921982\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02545977383852005\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08321043103933334\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0715087279677391\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024626541882753372\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.032885223627090454\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05332978814840317\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019648311659693718\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.048845984041690826\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07406765967607498\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05619779974222183\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.007287340238690376\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033874236047267914\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13395224511623383\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0749119222164154\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10056224465370178\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13275663554668427\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01820090226829052\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03454195335507393\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014371481724083424\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016633782535791397\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02681257762014866\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0738186240196228\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0796140804886818\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11522193998098373\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04013311490416527\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02878790721297264\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06263676285743713\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.06464260816574097\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03644583746790886\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04397065192461014\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04653511196374893\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08113149553537369\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02914796769618988\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04128815606236458\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029170149937272072\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02423788420855999\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.13730573654174805\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05758557468652725\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013249791227281094\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04218437895178795\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04931744188070297\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018260374665260315\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04320944473147392\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.015999924391508102\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014055880717933178\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.023893916979432106\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07632390409708023\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011274471879005432\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01418913621455431\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03254489228129387\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02383824996650219\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021362420171499252\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009915860369801521\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.031897809356451035\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010959822684526443\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01291970070451498\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02892458811402321\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.13455213606357574\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06383122503757477\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011393166147172451\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04370046406984329\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.011186694726347923\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028864813968539238\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010366820730268955\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.016419271007180214\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01664436236023903\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.0669938400387764\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.19168448448181152\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13907209038734436\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024854999035596848\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019171664491295815\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027230409905314445\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02489115111529827\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.030652787536382675\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019365748390555382\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02346133626997471\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014140589162707329\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09875497967004776\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07079759985208511\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.003917097579687834\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014104856178164482\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.027128120884299278\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05890338122844696\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.005084522068500519\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.047174300998449326\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0392925962805748\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03515502065420151\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07662132382392883\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08095449954271317\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.10363686084747314\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.044105492532253265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05009308457374573\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018948709592223167\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014645828865468502\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04749661311507225\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.10177617520093918\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03294936195015907\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02839924953877926\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04297434911131859\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.13950160145759583\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01992960460484028\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0964159220457077\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.0813836082816124\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06646367907524109\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.030805381014943123\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.009339912794530392\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.09900999814271927\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.038630809634923935\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06550244241952896\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08091200888156891\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12182863801717758\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.018508583307266235\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13227464258670807\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03239550441503525\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028388237580657005\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033153243362903595\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01823090761899948\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.042851850390434265\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.033829689025878906\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01320467609912157\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07996217161417007\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.028438996523618698\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.07305793464183807\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.0387076772749424\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.08103422820568085\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.014287585392594337\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.022816220298409462\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.05045139044523239\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04070979729294777\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.04740691930055618\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01568392850458622\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037968121469020844\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.12360391020774841\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.028479013592004776\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.06910345703363419\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07459469139575958\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.019444476813077927\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.07162953168153763\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.027343567460775375\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.13743452727794647\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03759615123271942\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.037825390696525574\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09843757003545761\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.02211201749742031\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06740380823612213\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013801941648125648\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.23972995579242706\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03691527247428894\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.025054924190044403\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03487654775381088\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.02741967886686325\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.013568315654993057\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.0377047136425972\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.029703769832849503\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.06034744530916214\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.11769583076238632\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.029165172949433327\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03580310568213463\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01392486784607172\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.066892109811306\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.023230068385601044\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.08130783587694168\n",
      "Training accuracy: 96.88\n",
      "Training loss: 0.09820391237735748\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.045584943145513535\n",
      "Training accuracy: 95.31\n",
      "Training loss: 0.08186407387256622\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.07741925865411758\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.010995522141456604\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.04089030623435974\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.024996811524033546\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.01811840385198593\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.04623239114880562\n",
      "Training accuracy: 98.44\n",
      "Training loss: 0.03787539154291153\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.021399416029453278\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.004903901368379593\n",
      "Training accuracy: 100.0\n",
      "Training loss: 0.03783586248755455\n",
      "Training accuracy: 93.75\n",
      "Training loss: 0.12542089819908142\n",
      "Validation accuracy: 97.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, model, loss_fn, optim_fn, training_data, test_data):\n",
    "    self.model = model.to(device)\n",
    "    self.loss_fn = loss_fn()\n",
    "    self.optim = optim_fn(self.model.parameters(), lr=0.001)\n",
    "    self.training_data = training_data\n",
    "    self.test_data = test_data\n",
    "\n",
    "  def train(self):\n",
    "    train_loader = DataLoader(self.training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "      self.optim.zero_grad()\n",
    "\n",
    "      data = data.squeeze(1).flatten(1,2)\n",
    "      data = data.to(device)\n",
    "      output = self.model(data)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      loss = self.loss_fn(output, targets)\n",
    "      loss.backward()\n",
    "\n",
    "      self.optim.step()\n",
    "\n",
    "      correct = torch.sum(output.argmax(1) == targets).item()\n",
    "      total = output.size(0)\n",
    "\n",
    "      print(f\"Training accuracy: {round((correct / total) * 100, 2)}\")\n",
    "      print(f\"Training loss: {loss.item()}\")\n",
    "\n",
    "  def validate(self):\n",
    "    test_loader = DataLoader(self.test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in test_loader:\n",
    "      data = data.squeeze(1).flatten(1,2)\n",
    "      data = data.to(device)\n",
    "\n",
    "      output = self.model(data)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      correct += torch.sum(output.argmax(1) == targets).item()\n",
    "      total += output.size(0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round((correct / total) * 100, 2)}\")\n",
    "\n",
    "trainer = Trainer(net, nn.CrossEntropyLoss, optim.Adam, training_data, test_data)\n",
    "\n",
    "epoch = 10\n",
    "for i in range(epoch):\n",
    "  print(f\"Epoch {i+1}..\")\n",
    "  trainer.train()\n",
    "  trainer.validate()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5c86d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "efbd25c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "model.load_state_dict(torch.load(\"model.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ad551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
